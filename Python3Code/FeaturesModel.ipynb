{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd6b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "#Loads a data file in the same directory and output in map output_features\n",
    "dataset_path = 'intermediate_datafiles/ML4QS_imputed_results.parquet.parquet'  # single source for both fourier and temporal\n",
    "output_dir = 'output_features'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "sampling_rate = 20                  # Hz sampling for FFT\n",
    "eps_length    = 256                 # FFT window length\n",
    "min_samples_per_recording = 10      # skip recordings shorter than this (samples)\n",
    "window_size   = 120                 # rolling window size for temporal\n",
    "\n",
    "def read_parquet(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    return df\n",
    "\n",
    "#first foureir\n",
    "def drop_unwanted_columns(df):\n",
    "    # Drop columns not needed for spectral analysis\n",
    "    drop_cols = [c for c in df.columns if (\n",
    "        'location' in c.lower() or\n",
    "        'proximity_phone_Distance' in c or\n",
    "        c.startswith('label_') or\n",
    "        c.startswith('transport_mode_') or\n",
    "        c.startswith('activity_')\n",
    "    )]\n",
    "    return df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "\n",
    "def add_recording_index(df, max_gap_ms=300):\n",
    "    recs = [0]\n",
    "    n = 0\n",
    "    for i in range(1, len(df)):\n",
    "        if (df.index[i] - df.index[i-1]) > timedelta(milliseconds=max_gap_ms):\n",
    "            n += 1\n",
    "        recs.append(n)\n",
    "    df.insert(0, 'recording_number', recs)\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_fft_features(signal, sampling_rate=1, fft_length=256):\n",
    "    vals = signal.dropna().values\n",
    "    if len(vals) == 0:\n",
    "        return pd.Series(dtype=float)\n",
    "    vals = np.pad(vals, (0, max(0, fft_length - len(vals))))[:fft_length]\n",
    "    freqs = np.fft.rfftfreq(fft_length, d=1/sampling_rate)\n",
    "    amps = np.abs(np.fft.rfft(vals))\n",
    "    max_f = freqs[np.argmax(amps)]\n",
    "    w_f = (freqs * amps).sum() / amps.sum() if amps.sum() > 0 else 0\n",
    "    psd = amps**2 / len(amps)\n",
    "    pdf = psd / psd.sum() if psd.sum() > 0 else psd\n",
    "    pse = -np.sum(np.log(pdf) * pdf) if np.all(pdf > 0) else 0\n",
    "    features = {'max_freq': max_f, 'freq_weighted': w_f, 'pse': pse}\n",
    "    features.update({f\"freq_{f:.3f}_Hz\": amp for f, amp in zip(freqs, amps)})\n",
    "    return pd.Series(features)\n",
    "\n",
    "\n",
    "def clean_fourier(df, n_bins=50):\n",
    "    base = pd.DataFrame(index=df.index)\n",
    "    binned = pd.DataFrame(index=df.index)\n",
    "    prefixes = sorted({c.split('_freq_')[0] for c in df.columns if '_freq_' in c})\n",
    "    for pre in prefixes:\n",
    "        cols = [c for c in df.columns if c.startswith(pre)]\n",
    "        for key in ['freq_0.000_Hz', 'freq_weighted', 'pse']:\n",
    "            cname = f\"{pre}_{key}\"\n",
    "            if cname in df:\n",
    "                base[cname] = df[cname]\n",
    "        amp_cols = [c for c in cols if re.match(rf\"{pre}_freq_\\d+\\.\\d*_Hz\", c)]\n",
    "        if amp_cols:\n",
    "            base[f\"{pre}_std\"] = df[amp_cols].std(axis=1)\n",
    "            freqs = [float(c.split('_freq_')[1].split('_Hz')[0]) for c in amp_cols]\n",
    "            edges = np.linspace(min(freqs), max(freqs), n_bins + 1)\n",
    "            for i in range(n_bins):\n",
    "                sel = [c for c, f in zip(amp_cols, freqs) if edges[i] <= f < edges[i+1]]\n",
    "                binned[f\"{pre}_band_{i}\"] = df[sel].mean(axis=1) if sel else 0\n",
    "    return pd.concat([base, binned], axis=1)\n",
    "\n",
    "#now the temporal features and magnitude etc\n",
    "def calculate_magnitude(df, prefix):\n",
    "    # Vector magnitude for 3-axis data\n",
    "    return np.sqrt(df[f\"{prefix}_X\"]**2 + df[f\"{prefix}_Y\"]**2 + df[f\"{prefix}_Z\"]**2)\n",
    "\n",
    "def temporal_aggregation(series, window_size=120, aggs=['mean','median','min','max','std']):\n",
    "    out = pd.DataFrame(index=series.index)\n",
    "    for func in aggs:\n",
    "        if func == 'std':\n",
    "            out[f\"{series.name}_{func}\"] = series.rolling(window_size, min_periods=1).std().fillna(0)\n",
    "        else:\n",
    "            out[f\"{series.name}_{func}\"] = series.rolling(window_size, min_periods=1).agg(func)\n",
    "    return out\n",
    "\n",
    "def calculate_direction_changes(s, thresh=10):\n",
    "    # Flag > thresh-degree jumps, accounting for circular wrap\n",
    "    d = s.diff().fillna(0)\n",
    "    d = np.where(d>180, d-360, np.where(d<-180, d+360, d))\n",
    "    return pd.Series((abs(d) > thresh).astype(int), index=s.index)\n",
    "\n",
    "def calculate_switches(mag, thresh=0.1):\n",
    "    # Count sign changes in thresholded diff\n",
    "    d = mag.diff().fillna(0)\n",
    "    state = np.where(d > thresh, 1, np.where(d < -thresh, -1, 0))\n",
    "    sw = np.abs(np.diff(state, prepend=state[0])) > 0\n",
    "    return pd.Series(sw.astype(int), index=mag.index)\n",
    "\n",
    "def process_temporal(df, window_size=120):\n",
    "    sessions = []\n",
    "    for sid, grp in df.groupby('id'):\n",
    "        g = grp.copy()\n",
    "        # Compute magnitudes\n",
    "        for p in ['acc_phone','lin_acc_phone','gyr_phone']:\n",
    "            g[f\"{p}_magnitude\"] = calculate_magnitude(g, p)\n",
    "        # Compute direction and acceleration switches\n",
    "        g['direction_changes'] = calculate_direction_changes(g['location_phone_Direction'])\n",
    "        for p in ['acc_phone_magnitude','lin_acc_phone_magnitude','gyr_phone_magnitude']:\n",
    "            key = p.replace('_magnitude','')\n",
    "            g[f\"{key}_switches\"] = calculate_switches(g[p])\n",
    "        # Rolling aggregations\n",
    "        features = [\n",
    "            'acc_phone_X','acc_phone_Y','acc_phone_Z',\n",
    "            'lin_acc_phone_X','lin_acc_phone_Y','lin_acc_phone_Z',\n",
    "            'gyr_phone_X','gyr_phone_Y','gyr_phone_Z',\n",
    "            'location_phone_Velocity','location_phone_Direction',\n",
    "            'location_phone_Horizontal Accuracy','location_phone_Vertical Accuracy',\n",
    "            'acc_phone_magnitude','lin_acc_phone_magnitude','gyr_phone_magnitude',\n",
    "            'direction_changes','acc_phone_switches','lin_acc_phone_switches','gyr_phone_switches'\n",
    "        ]\n",
    "        for feat in features:\n",
    "            if feat in g:\n",
    "                g = pd.concat([g, temporal_aggregation(g[feat], window_size)], axis=1)\n",
    "        # Session-level stats: use the correct switch column names\n",
    "        switch_cols = ['direction_changes','acc_phone_switches','lin_acc_phone_switches','gyr_phone_switches']\n",
    "        rates = {f\"session_{c}_rate\": g[c].mean() for c in switch_cols}\n",
    "        rates['session_avg_velocity'] = g['location_phone_Velocity'].mean()\n",
    "        rates['session_velocity_std'] = g['location_phone_Velocity'].std()\n",
    "        for k, v in rates.items():\n",
    "            g[k] = v\n",
    "        sessions.append(g)\n",
    "    return pd.concat(sessions)\n",
    "\n",
    "# Load and branch raw data for fourier and temporal (clean only before FFT)\n",
    "raw_fft = read_parquet(dataset_path)\n",
    "raw_fft = drop_unwanted_columns(raw_fft)  # only affects spectral branch\n",
    "raw_fft = raw_fft.dropna()\n",
    "raw_fft = add_recording_index(raw_fft)\n",
    "\n",
    "raw_temp = read_parquet(dataset_path)       # full data for temporal branch\n",
    "\n",
    "fft_rows = []\n",
    "for rid, grp in raw_fft.groupby('id'):\n",
    "    if len(grp) < min_samples_per_recording: continue\n",
    "    feats = {}\n",
    "    for col in grp.select_dtypes(include='number').columns.drop('recording_number'):\n",
    "        s = compute_fft_features(grp[col], sampling_rate, eps_length)\n",
    "        s.index = [f\"{col}_{i}\" for i in s.index]\n",
    "        feats.update(s.to_dict())\n",
    "    feats['id'] = rid\n",
    "    fft_rows.append(feats)\n",
    "\n",
    "\n",
    "df_fft = pd.DataFrame(fft_rows).set_index('id')\n",
    "fft_clean = clean_fourier(df_fft)\n",
    "\n",
    "temp = process_temporal(raw_temp, window_size)\n",
    "if 'id' in temp.columns:\n",
    "    temp = temp.set_index('id')\n",
    "\n",
    "# Merge on 'id'\n",
    "merged = fft_clean.join(\n",
    "    temp,                   # temporal features\n",
    "    how='inner',            # only matching ids\n",
    "    rsuffix='_temp'         # suffix for any overlapping column names\n",
    ")\n",
    "\n",
    "# Save unified features\n",
    "# This CSV now contains all FFT-based and rolling temporal features per session (id)\n",
    "combined_path = os.path.join(output_dir, 'combined_features.csv')\n",
    "merged.to_csv(combined_path)\n",
    "print(f\"Combined features saved to {combined_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae86978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "df = pd.read_csv(\"output_features/combined_features.csv\")\n",
    "\n",
    "print(\"Original DataFrame shape:\", df.shape)\n",
    "print(\"Available columns before feature removal:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Define leaking features to remove\n",
    "leaking_features = [\n",
    "    'location_phone_Latitude',\n",
    "    'location_phone_Longitude', \n",
    "    'location_phone_Direction',\n",
    "    'location_phone_Velocity',\n",
    "    'proximity_phone_Distance',\n",
    "    'location_phone_Horizontal Accuracy',\n",
    "    'location_phone_Height',\n",
    "    'location_phone_Vertical Accuracy'\n",
    "]\n",
    "\n",
    "# Remove leaking features\n",
    "print(f\"\\nRemoving {len(leaking_features)} leaking features...\")\n",
    "features_found = [f for f in leaking_features if f in df.columns]\n",
    "features_not_found = [f for f in leaking_features if f not in df.columns]\n",
    "\n",
    "if features_found:\n",
    "    print(f\"Removing features: {features_found}\")\n",
    "    df = df.drop(columns=features_found)\n",
    "else:\n",
    "    print(\"No leaking features found in the dataset\")\n",
    "\n",
    "if features_not_found:\n",
    "    print(f\"Features not found (already removed?): {features_not_found}\")\n",
    "\n",
    "print(f\"DataFrame shape after removing leaking features: {df.shape}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Inspect remaining columns\n",
    "# -------------------------------\n",
    "print(\"\\nRemaining columns:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "\n",
    "# -------------------------------\n",
    "# FIND ALL LABEL COLUMNS AUTOMATICALLY\n",
    "# -------------------------------\n",
    "# Find all label columns (assuming they start with 'label')\n",
    "label_columns = [col for col in df.columns if col.startswith('label')]\n",
    "print(f\"\\nFound label columns: {label_columns}\")\n",
    "\n",
    "# Extract transportation modes from label column names\n",
    "transportation_modes = [col.replace('label', '') for col in label_columns]\n",
    "print(f\"Transportation modes: {transportation_modes}\")\n",
    "\n",
    "def create_target_label(row):\n",
    "    \"\"\"Create target label from all available label columns\"\"\"\n",
    "    for i, col in enumerate(label_columns):\n",
    "        if row.get(col, 0) == 1:\n",
    "            return transportation_modes[i]\n",
    "    return 'unknown'\n",
    "\n",
    "df['transport_mode'] = df.apply(create_target_label, axis=1)\n",
    "\n",
    "# Show all available transport modes and their counts\n",
    "print(f\"\\nAll transport modes found in data:\")\n",
    "mode_counts = df['transport_mode'].value_counts()\n",
    "print(mode_counts)\n",
    "\n",
    "# Filter out 'unknown' samples (if any)\n",
    "df = df[df['transport_mode'] != 'unknown']\n",
    "\n",
    "print(f\"\\nAfter filtering out unknown samples: {df.shape[0]} samples\")\n",
    "print(f\"Final class distribution:\\n{df['transport_mode'].value_counts()}\")\n",
    "\n",
    "# Check session distribution by transport mode\n",
    "print(f\"\\nSession distribution analysis:\")\n",
    "session_counts = df.groupby(['id', 'transport_mode']).size().unstack(fill_value=0)\n",
    "\n",
    "sessions_per_mode = {}\n",
    "for mode in df['transport_mode'].unique():\n",
    "    sessions_with_mode = (session_counts[mode] > 0).sum() if mode in session_counts.columns else 0\n",
    "    sessions_per_mode[mode] = sessions_with_mode\n",
    "    print(f\"{mode} sessions: {sessions_with_mode}\")\n",
    "\n",
    "print(f\"Total unique sessions: {df['id'].nunique()}\")\n",
    "\n",
    "# Warn about modes with very few sessions\n",
    "print(f\"\\n Session distribution warnings:\")\n",
    "for mode, count in sessions_per_mode.items():\n",
    "    if count < 3:\n",
    "        print(f\"WARNING: {mode} has only {count} session(s) - may cause issues in train/test split\")\n",
    "\n",
    "# -------------------------------\n",
    "# Feature selection\n",
    "# -------------------------------\n",
    "# Find timestamp column (if exists)\n",
    "time_like_columns = [col for col in df.columns if any(keyword in col.lower() for keyword in ['time', 'date', 'stamp'])]\n",
    "timestamp_col = None\n",
    "possible_timestamp_cols = ['timestamp', 'time', 'datetime', 'date_time', 'session_time']\n",
    "\n",
    "for col in possible_timestamp_cols:\n",
    "    if col in df.columns:\n",
    "        timestamp_col = col\n",
    "        break\n",
    "\n",
    "if timestamp_col is None and time_like_columns:\n",
    "    timestamp_col = time_like_columns[0]\n",
    "\n",
    "# Ensure timestamp column is excluded from features to prevent data leakage\n",
    "exclude_cols = ['id', 'transport_mode']\n",
    "if timestamp_col:\n",
    "    exclude_cols.append(timestamp_col)\n",
    "\n",
    "feature_cols = [\n",
    "    col for col in df.columns\n",
    "    if not col.startswith('label')\n",
    "    and col not in exclude_cols\n",
    "]\n",
    "\n",
    "print(f\"\\nUsing {len(feature_cols)} features (after removing leaking features)\")\n",
    "if timestamp_col:\n",
    "    print(f\"Timestamp column excluded: '{timestamp_col}'\")\n",
    "\n",
    "# Verify no leaking features remain\n",
    "remaining_leaking = [f for f in leaking_features if f in feature_cols]\n",
    "if remaining_leaking:\n",
    "    print(f\"Some leaking features still present: {remaining_leaking}\")\n",
    "else:\n",
    "    print(\"No leaking features in final feature set\")\n",
    "\n",
    "# -------------------------------\n",
    "# Handle non-numeric columns (especially timedelta)\n",
    "# -------------------------------\n",
    "print(f\"\\nChecking data types in features...\")\n",
    "print(f\"Feature dtypes (first 10):\")\n",
    "for col in feature_cols[:10]:\n",
    "    if col in df.columns:\n",
    "        dtype = df[col].dtype\n",
    "        print(f\"  {col}: {dtype}\")\n",
    "        \n",
    "        if dtype == 'object':\n",
    "            sample_values = df[col].dropna().head(3).tolist()\n",
    "            print(f\"    Sample values: {sample_values}\")\n",
    "\n",
    "# Convert timedelta columns to numeric (seconds)\n",
    "timedelta_cols = []\n",
    "problematic_cols = []\n",
    "\n",
    "for col in feature_cols:\n",
    "    if col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            sample_val = str(df[col].dropna().iloc[0]) if len(df[col].dropna()) > 0 else \"\"\n",
    "            if 'days' in sample_val and ':' in sample_val:\n",
    "                print(f\"Converting timedelta column '{col}' to seconds...\")\n",
    "                try:\n",
    "                    df[col] = pd.to_timedelta(df[col]).dt.total_seconds()\n",
    "                    timedelta_cols.append(col)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to convert {col}: {e}\")\n",
    "                    problematic_cols.append(col)\n",
    "\n",
    "if timedelta_cols:\n",
    "    print(f\"Converted {len(timedelta_cols)} timedelta columns to seconds: {timedelta_cols}\")\n",
    "\n",
    "# Remove non-numeric columns\n",
    "non_numeric_cols = []\n",
    "for col in feature_cols:\n",
    "    if col in df.columns:\n",
    "        if df[col].dtype == 'object' or not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            non_numeric_cols.append(col)\n",
    "\n",
    "non_numeric_cols.extend(problematic_cols)\n",
    "non_numeric_cols = list(set(non_numeric_cols))\n",
    "\n",
    "if non_numeric_cols:\n",
    "    print(f\"Found non-numeric columns that will be excluded: {non_numeric_cols}\")\n",
    "    feature_cols = [col for col in feature_cols if col not in non_numeric_cols]\n",
    "    print(f\"Updated feature count: {len(feature_cols)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# IMPROVED SESSION-LEVEL SPLIT FOR ALL CLASSES\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"IMPLEMENTING SESSION-LEVEL SPLIT FOR ALL TRANSPORT MODES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get unique session IDs for each transport mode\n",
    "session_split_info = {}\n",
    "all_train_sessions = []\n",
    "all_test_sessions = []\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "for mode in df['transport_mode'].unique():\n",
    "    mode_sessions = df[df['transport_mode'] == mode]['id'].unique()\n",
    "    mode_sessions_shuffled = np.random.permutation(mode_sessions)\n",
    "    \n",
    "    n_sessions = len(mode_sessions_shuffled)\n",
    "    \n",
    "    # Handle cases with very few sessions\n",
    "    if n_sessions == 1:\n",
    "        # If only 1 session, put it in training\n",
    "        train_sessions = mode_sessions_shuffled\n",
    "        test_sessions = []\n",
    "        print(f\"{mode}: Only 1 session - putting in training set\")\n",
    "    elif n_sessions == 2:\n",
    "        # If only 2 sessions, put 1 in each\n",
    "        train_sessions = mode_sessions_shuffled[:1]\n",
    "        test_sessions = mode_sessions_shuffled[1:]\n",
    "        print(f\"{mode}: Only 2 sessions - 1 train, 1 test\")\n",
    "    else:\n",
    "        # Normal 80/20 split\n",
    "        split_idx = max(1, int(0.8 * n_sessions))  # Ensure at least 1 in training\n",
    "        train_sessions = mode_sessions_shuffled[:split_idx]\n",
    "        test_sessions = mode_sessions_shuffled[split_idx:]\n",
    "    \n",
    "    session_split_info[mode] = {\n",
    "        'total': n_sessions,\n",
    "        'train': len(train_sessions),\n",
    "        'test': len(test_sessions)\n",
    "    }\n",
    "    \n",
    "    all_train_sessions.extend(train_sessions)\n",
    "    all_test_sessions.extend(test_sessions)\n",
    "\n",
    "print(f\"\\nSession split breakdown:\")\n",
    "for mode, info in session_split_info.items():\n",
    "    print(f\"  {mode}: {info['total']} total -> {info['train']} train, {info['test']} test\")\n",
    "\n",
    "print(f\"\\nTotal sessions:\")\n",
    "print(f\"  Train: {len(all_train_sessions)}\")\n",
    "print(f\"  Test: {len(all_test_sessions)}\")\n",
    "\n",
    "# Create train and test datasets\n",
    "df_train = df[df['id'].isin(all_train_sessions)].copy()\n",
    "df_test = df[df['id'].isin(all_test_sessions)].copy()\n",
    "\n",
    "# Verify no session overlap\n",
    "train_session_set = set(df_train['id'].unique())\n",
    "test_session_set = set(df_test['id'].unique())\n",
    "session_overlap = train_session_set.intersection(test_session_set)\n",
    "\n",
    "print(f\"\\nðŸ” VERIFICATION:\")\n",
    "print(f\"Train sessions: {len(train_session_set)}\")\n",
    "print(f\"Test sessions: {len(test_session_set)}\")\n",
    "print(f\"Session overlap: {len(session_overlap)}\")\n",
    "\n",
    "if len(session_overlap) == 0:\n",
    "    print(\"No session overlap between train and test!\")\n",
    "else:\n",
    "    print(\"Session overlap detected!\")\n",
    "\n",
    "# Prepare features and targets\n",
    "X_train = df_train[feature_cols]\n",
    "X_test = df_test[feature_cols]\n",
    "y_train = df_train['transport_mode']\n",
    "y_test = df_test['transport_mode']\n",
    "\n",
    "print(f\"\\nFinal dataset sizes:\")\n",
    "print(f\"Train samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Train class distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"Test class distribution:\\n{y_test.value_counts()}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nData quality check:\")\n",
    "print(f\"Missing values in train features: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"Infinite values in train features: {np.isinf(X_train.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# Final cleanup of any remaining object columns\n",
    "object_cols = X_train.select_dtypes(include=['object']).columns\n",
    "if len(object_cols) > 0:\n",
    "    print(f\"Removing remaining object columns: {list(object_cols)}\")\n",
    "    X_train = X_train.select_dtypes(exclude=['object'])\n",
    "    X_test = X_test.select_dtypes(exclude=['object'])\n",
    "\n",
    "print(f\"Final feature matrix shape: Train {X_train.shape}, Test {X_test.shape}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Scaler features which for XGBOOST not need but for other maybe\n",
    "# -------------------------------\n",
    "print(f\"\\nStarting preprocessing...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Label encoding for multi-class classification\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "print(f\"\\nLabel encoding for {len(label_encoder.classes_)}-class classification:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {class_name} = {i}\")\n",
    "\n",
    "print(f\"Train labels distribution: {np.bincount(y_train_encoded)}\")\n",
    "print(f\"Test labels distribution: {np.bincount(y_test_encoded)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# TRAIN XGBOOS|T\n",
    "# -------------------------------\n",
    "dtrain = xgb.DMatrix(X_train_scaled, label=y_train_encoded)\n",
    "dtest = xgb.DMatrix(X_test_scaled, label=y_test_encoded)\n",
    "\n",
    "# Calculate scale_pos_weight for class imbalance (for multi-class, we'll use different approach)\n",
    "class_counts = np.bincount(y_train_encoded)\n",
    "n_classes = len(label_encoder.classes_)\n",
    "\n",
    "params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': n_classes,\n",
    "    'max_depth': 6,  # Reduced from 8 to prevent overfitting with more classes\n",
    "    'learning_rate': 0.05,  # Slightly higher learning rate\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'reg_alpha': 0.1,  # L1 regularization\n",
    "    'reg_lambda': 1.0   # L2 regularization\n",
    "}\n",
    "\n",
    "print(f\"\\nTraining XGBoost model for {n_classes}-class classification...\")\n",
    "print(f\"Classes: {list(label_encoder.classes_)}\")\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=20,  # Increased rounds for more complex problem\n",
    "    evals=[(dtrain, 'train'), (dtest, 'eval')],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluation\n",
    "# -------------------------------\n",
    "y_pred_proba = model.predict(dtest)\n",
    "y_pred_encoded = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Convert back to original labels\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_encoded)\n",
    "y_test_labels = label_encoder.inverse_transform(y_test_encoded)\n",
    "\n",
    "accuracy = accuracy_score(y_test_labels, y_pred_labels)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels, zero_division=0))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "unique_labels = sorted(label_encoder.classes_)\n",
    "cm = confusion_matrix(y_test_labels, y_pred_labels, labels=unique_labels)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=unique_labels, \n",
    "            yticklabels=unique_labels)\n",
    "plt.title(f'Confusion Matrix: {n_classes}-Class Transportation Mode Classification')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Feature Importance\n",
    "# -------------------------------\n",
    "feature_importance = model.get_score(importance_type='weight')\n",
    "final_feature_names = X_train.columns.tolist()\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': final_feature_names,\n",
    "    'importance': [feature_importance.get(f'f{i}', 0) for i in range(len(final_feature_names))]\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = importance_df.head(20)\n",
    "sns.barplot(data=top_features, x='importance', y='feature')\n",
    "plt.title(f'Top 20 Most Important Features: {n_classes}-Class Classification')\n",
    "plt.xlabel('Feature Importance (Weight)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMost Important Features:\")\n",
    "print(importance_df.head(16))\n",
    "\n",
    "# -------------------------------\n",
    "# Additional Analysis\n",
    "# -------------------------------\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"ADDITIONAL ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Per-class accuracy analysis\n",
    "print(f\"\\nPer-class Performance Analysis:\")\n",
    "for class_name in label_encoder.classes_:\n",
    "    class_mask = y_test_labels == class_name\n",
    "    n_samples = np.sum(class_mask)\n",
    "    if n_samples > 0:\n",
    "        class_accuracy = (y_pred_labels[class_mask] == class_name).mean()\n",
    "        print(f\"  {class_name:>12}: {class_accuracy:.4f} accuracy ({n_samples:>4} samples)\")\n",
    "    else:\n",
    "        print(f\"  {class_name:>12}: No test samples\")\n",
    "\n",
    "# Show prediction confidence distribution\n",
    "max_probabilities = np.max(y_pred_proba, axis=1)\n",
    "print(f\"\\nPrediction Confidence Distribution:\")\n",
    "print(f\"Mean confidence: {max_probabilities.mean():.4f}\")\n",
    "print(f\"Std confidence: {max_probabilities.std():.4f}\")\n",
    "print(f\"Min confidence: {max_probabilities.min():.4f}\")\n",
    "print(f\"Max confidence: {max_probabilities.max():.4f}\")\n",
    "\n",
    "# Sample predictions with probabilities\n",
    "print(f\"\\nSample Predictions (showing top 3 probabilities for each):\")\n",
    "sample_indices = np.random.choice(len(y_test_labels), min(10, len(y_test_labels)), replace=False)\n",
    "for i in sample_indices:\n",
    "    true_label = y_test_labels[i]\n",
    "    pred_label = y_pred_labels[i]\n",
    "    \n",
    "    # Get top 3 predictions for this sample\n",
    "    top_3_indices = np.argsort(y_pred_proba[i])[-3:][::-1]\n",
    "    top_3_probs = [(label_encoder.classes_[idx], y_pred_proba[i][idx]) for idx in top_3_indices]\n",
    "    \n",
    "    print(f\"True: {true_label:>12}, Pred: {pred_label:>12}\")\n",
    "    print(f\"  Top 3: {top_3_probs[0][0]}({top_3_probs[0][1]:.3f}), {top_3_probs[1][0]}({top_3_probs[1][1]:.3f}), {top_3_probs[2][0]}({top_3_probs[2][1]:.3f})\")\n",
    "\n",
    "print(f\"\\nSumary:\")\n",
    "print(f\"Multi-class classification accuracy: {accuracy:.4f}\")\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "print(f\"Classes: {list(label_encoder.classes_)}\")\n",
    "print(f\"Features used: {X_train.shape[1]}\")\n",
    "print(f\"Total samples: {len(df)} (Train: {len(df_train)}, Test: {len(df_test)})\")\n",
    "\n",
    "# Identify problematic classes (those with very low performance)\n",
    "print(f\"\\nClasses that may need attention:\")\n",
    "for class_name in label_encoder.classes_:\n",
    "    class_mask = y_test_labels == class_name\n",
    "    n_samples = np.sum(class_mask)\n",
    "    if n_samples > 0:\n",
    "        class_accuracy = (y_pred_labels[class_mask] == class_name).mean()\n",
    "        if class_accuracy < 0.5:\n",
    "            print(f\"  {class_name}: {class_accuracy:.4f} accuracy (low performance)\")\n",
    "        elif n_samples < 10:\n",
    "            print(f\"  {class_name}: Only {n_samples} test samples (may need more data)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
