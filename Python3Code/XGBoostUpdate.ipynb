{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b058571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "STUDENT = 'yoric'\n",
    "\n",
    "OUTLIERS_PATH = Path(r'C:\\Users\\yoric\\ML4QS\\ML4QS-GROUP99-Michael-FeatureEng\\Python3Code\\outliers2')\n",
    "INTERMEDIATE_PATH = Path(r'C:\\Users\\yoric\\ML4QS\\ML4QS-GROUP99-Michael-FeatureEng\\Python3Code\\intermediate_datafiles')\n",
    "\n",
    "os.chdir(r'C:\\Users\\yoric\\ML4QS\\ML4QS-GROUP99-Michael-FeatureEng\\Python3Code')\n",
    "\n",
    "EXPERIMENT_DIR = 'ML4QS-Vehicle-2'\n",
    "\n",
    "from util.VisualizeDataset import VisualizeDataset\n",
    "from Visualiser import Visualiser as Viz\n",
    "from outlier_detector import OutlierDetector\n",
    "from custom_imputer import CustomImputer\n",
    "from util.util import ignore_actual_time, read_parquet, write_parquet\n",
    "from DataLoader import PhyboxDatasetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c8696d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intermediate df\n",
    "intermediate_df = pd.read_parquet(INTERMEDIATE_PATH / 'ML4QS_imputed_results.parquet')\n",
    "\n",
    "# Define leaking features to remove\n",
    "leaking_features = [\n",
    "    'location_phone_Latitude',\n",
    "    'location_phone_Longitude', \n",
    "    'proximity_phone_Distance',\n",
    "    'location_phone_Horizontal Accuracy',\n",
    "    'location_phone_Height'\n",
    "]\n",
    "\n",
    "# Remove leaking features\n",
    "intermediate_df = intermediate_df.drop(columns=leaking_features, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8adc513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def calculate_magnitude(df, prefix):\n",
    "    x_col = f\"{prefix}_X\"\n",
    "    y_col = f\"{prefix}_Y\" \n",
    "    z_col = f\"{prefix}_Z\"\n",
    "    return np.sqrt(df[x_col]**2 + df[y_col]**2 + df[z_col]**2)\n",
    "\n",
    "def temporal_aggregation_no_leak(series, window_size=120, agg_functions=['mean', 'median', 'min', 'max', 'std']):\n",
    "    result = pd.DataFrame()\n",
    "    for func in agg_functions:\n",
    "        if func == 'std':\n",
    "            result[f\"{series.name}_{func}\"] = series.rolling(\n",
    "                window=window_size, min_periods=1\n",
    "            ).std().shift(1).fillna(0)\n",
    "        else:\n",
    "            result[f\"{series.name}_{func}\"] = series.rolling(\n",
    "                window=window_size, min_periods=1\n",
    "            ).agg(func).shift(1).fillna(method='bfill')\n",
    "    return result\n",
    "\n",
    "def calculate_direction_changes(direction_series, threshold=10):\n",
    "    direction_diff = direction_series.diff().fillna(0)\n",
    "    direction_diff = np.where(direction_diff > 180, direction_diff - 360, direction_diff)\n",
    "    direction_diff = np.where(direction_diff < -180, direction_diff + 360, direction_diff)\n",
    "    significant_changes = np.abs(direction_diff) > threshold\n",
    "    return pd.Series(significant_changes.astype(int), index=direction_series.index)\n",
    "\n",
    "def calculate_acceleration_switches(acceleration_magnitude, threshold=0.1):\n",
    "    acc_diff = acceleration_magnitude.diff().fillna(0)\n",
    "    acc_state = np.where(acc_diff > threshold, 1,  \n",
    "                       np.where(acc_diff < -threshold, -1, 0))  \n",
    "    switches = np.abs(np.diff(acc_state, prepend=acc_state[0])) > 0\n",
    "    return pd.Series(switches.astype(int), index=acceleration_magnitude.index)\n",
    "\n",
    "def process_transportation_data(df, window_size=120):\n",
    "    processed_sessions = []\n",
    "\n",
    "    for session_id, session_data in df.groupby('id'):\n",
    "        session_df = session_data.copy()\n",
    "        \n",
    "        session_df['acc_phone_magnitude'] = calculate_magnitude(session_df, 'acc_phone')\n",
    "        session_df['lin_acc_phone_magnitude'] = calculate_magnitude(session_df, 'lin_acc_phone') \n",
    "        session_df['gyr_phone_magnitude'] = calculate_magnitude(session_df, 'gyr_phone')\n",
    "        \n",
    "        session_df['direction_changes'] = calculate_direction_changes(session_df['location_phone_Direction'])\n",
    "        session_df['acc_switches'] = calculate_acceleration_switches(session_df['acc_phone_magnitude'])\n",
    "        session_df['lin_acc_switches'] = calculate_acceleration_switches(session_df['lin_acc_phone_magnitude'])\n",
    "        session_df['rotation_switches'] = calculate_acceleration_switches(session_df['gyr_phone_magnitude'])\n",
    "        \n",
    "        features_to_aggregate = [\n",
    "            'acc_phone_X', 'acc_phone_Y', 'acc_phone_Z',\n",
    "            'lin_acc_phone_X', 'lin_acc_phone_Y', 'lin_acc_phone_Z',\n",
    "            'gyr_phone_X', 'gyr_phone_Y', 'gyr_phone_Z',\n",
    "            'location_phone_Velocity', 'location_phone_Direction',\n",
    "            'acc_phone_magnitude', 'lin_acc_phone_magnitude', 'gyr_phone_magnitude',\n",
    "            'direction_changes', 'acc_switches', 'lin_acc_switches', 'rotation_switches'\n",
    "        ]\n",
    "        \n",
    "        for feature in features_to_aggregate:\n",
    "            if feature in session_df.columns:\n",
    "                aggregated = temporal_aggregation_no_leak(session_df[feature], window_size)\n",
    "                session_df = pd.concat([session_df, aggregated], axis=1)\n",
    "        \n",
    "        session_df['session_direction_change_rate'] = session_df['direction_changes'].mean()\n",
    "        session_df['session_acc_switch_rate'] = session_df['acc_switches'].mean()\n",
    "        session_df['session_lin_acc_switch_rate'] = session_df['lin_acc_switches'].mean()\n",
    "        session_df['session_rotation_switch_rate'] = session_df['rotation_switches'].mean()\n",
    "        session_df['session_avg_velocity'] = session_df['location_phone_Velocity'].mean()\n",
    "        session_df['session_velocity_std'] = session_df['location_phone_Velocity'].std()\n",
    "        \n",
    "        processed_sessions.append(session_df)\n",
    "\n",
    "    result_df = pd.concat(processed_sessions, ignore_index=True)\n",
    "    return result_df\n",
    "\n",
    "# === Example usage ===\n",
    "# Load your intermediate dataset\n",
    "# intermediate_df = pd.read_parquet('path/to/your/intermediate_file.parquet')\n",
    "\n",
    "# Process the data without Fourier features\n",
    "# session_features = process_transportation_data(intermediate_df, window_size=120)\n",
    "\n",
    "# Save the processed dataset\n",
    "#session_features.to_csv('C:/Users/yoric/ML4QS/session_features_imputed.csv', index=False)\n",
    "\n",
    "# Load your intermediate dataset\n",
    "#intermediate_df = pd.read_parquet('C:/Users/yoric/ML4QS/intermediate_df.parquet')\n",
    "\n",
    "# Process the data\n",
    "session_features = process_transportation_data(intermediate_df, window_size=120)\n",
    "\n",
    "# Save the result\n",
    "session_features.to_csv('C:/Users/yoric/ML4QS/session_features_imputed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "311322a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (53043, 130)\n",
      "Available columns before feature removal:\n",
      "['timestamp', 'id', 'acc_phone_X', 'acc_phone_Y', 'acc_phone_Z', 'lin_acc_phone_X', 'lin_acc_phone_Y', 'lin_acc_phone_Z', 'gyr_phone_X', 'gyr_phone_Y', 'gyr_phone_Z', 'location_phone_Velocity', 'location_phone_Direction', 'location_phone_Vertical Accuracy', 'mag_phone_X', 'mag_phone_Y', 'mag_phone_Z', 'labeltram', 'labeltrain', 'labelwalking', 'labelmetro', 'labelbus', 'labelcar', 'labelbike', 'original_time', 'time_diff', 'shifted_time', 'acc_phone_magnitude', 'lin_acc_phone_magnitude', 'gyr_phone_magnitude', 'direction_changes', 'acc_switches', 'lin_acc_switches', 'rotation_switches', 'acc_phone_X_mean', 'acc_phone_X_median', 'acc_phone_X_min', 'acc_phone_X_max', 'acc_phone_X_std', 'acc_phone_Y_mean', 'acc_phone_Y_median', 'acc_phone_Y_min', 'acc_phone_Y_max', 'acc_phone_Y_std', 'acc_phone_Z_mean', 'acc_phone_Z_median', 'acc_phone_Z_min', 'acc_phone_Z_max', 'acc_phone_Z_std', 'lin_acc_phone_X_mean', 'lin_acc_phone_X_median', 'lin_acc_phone_X_min', 'lin_acc_phone_X_max', 'lin_acc_phone_X_std', 'lin_acc_phone_Y_mean', 'lin_acc_phone_Y_median', 'lin_acc_phone_Y_min', 'lin_acc_phone_Y_max', 'lin_acc_phone_Y_std', 'lin_acc_phone_Z_mean', 'lin_acc_phone_Z_median', 'lin_acc_phone_Z_min', 'lin_acc_phone_Z_max', 'lin_acc_phone_Z_std', 'gyr_phone_X_mean', 'gyr_phone_X_median', 'gyr_phone_X_min', 'gyr_phone_X_max', 'gyr_phone_X_std', 'gyr_phone_Y_mean', 'gyr_phone_Y_median', 'gyr_phone_Y_min', 'gyr_phone_Y_max', 'gyr_phone_Y_std', 'gyr_phone_Z_mean', 'gyr_phone_Z_median', 'gyr_phone_Z_min', 'gyr_phone_Z_max', 'gyr_phone_Z_std', 'location_phone_Velocity_mean', 'location_phone_Velocity_median', 'location_phone_Velocity_min', 'location_phone_Velocity_max', 'location_phone_Velocity_std', 'location_phone_Direction_mean', 'location_phone_Direction_median', 'location_phone_Direction_min', 'location_phone_Direction_max', 'location_phone_Direction_std', 'acc_phone_magnitude_mean', 'acc_phone_magnitude_median', 'acc_phone_magnitude_min', 'acc_phone_magnitude_max', 'acc_phone_magnitude_std', 'lin_acc_phone_magnitude_mean', 'lin_acc_phone_magnitude_median', 'lin_acc_phone_magnitude_min', 'lin_acc_phone_magnitude_max', 'lin_acc_phone_magnitude_std', 'gyr_phone_magnitude_mean', 'gyr_phone_magnitude_median', 'gyr_phone_magnitude_min', 'gyr_phone_magnitude_max', 'gyr_phone_magnitude_std', 'direction_changes_mean', 'direction_changes_median', 'direction_changes_min', 'direction_changes_max', 'direction_changes_std', 'acc_switches_mean', 'acc_switches_median', 'acc_switches_min', 'acc_switches_max', 'acc_switches_std', 'lin_acc_switches_mean', 'lin_acc_switches_median', 'lin_acc_switches_min', 'lin_acc_switches_max', 'lin_acc_switches_std', 'rotation_switches_mean', 'rotation_switches_median', 'rotation_switches_min', 'rotation_switches_max', 'rotation_switches_std', 'session_direction_change_rate', 'session_acc_switch_rate', 'session_lin_acc_switch_rate', 'session_rotation_switch_rate', 'session_avg_velocity', 'session_velocity_std']\n",
      "\n",
      "Removing 10 leaking features...\n",
      "Removing features: ['location_phone_Direction', 'location_phone_Velocity', 'location_phone_Vertical Accuracy', 'lin_acc_phone_Z', 'lin_acc_phone_Y']\n",
      "Features not found (already removed?): ['location_phone_Latitude', 'location_phone_Longitude', 'proximity_phone_Distance', 'location_phone_Horizontal Accuracy', 'location_phone_Height']\n",
      "DataFrame shape after removing leaking features: (53043, 125)\n",
      "\n",
      "Remaining columns:\n",
      "['timestamp', 'id', 'acc_phone_X', 'acc_phone_Y', 'acc_phone_Z', 'lin_acc_phone_X', 'gyr_phone_X', 'gyr_phone_Y', 'gyr_phone_Z', 'mag_phone_X', 'mag_phone_Y', 'mag_phone_Z', 'labeltram', 'labeltrain', 'labelwalking', 'labelmetro', 'labelbus', 'labelcar', 'labelbike', 'original_time', 'time_diff', 'shifted_time', 'acc_phone_magnitude', 'lin_acc_phone_magnitude', 'gyr_phone_magnitude', 'direction_changes', 'acc_switches', 'lin_acc_switches', 'rotation_switches', 'acc_phone_X_mean', 'acc_phone_X_median', 'acc_phone_X_min', 'acc_phone_X_max', 'acc_phone_X_std', 'acc_phone_Y_mean', 'acc_phone_Y_median', 'acc_phone_Y_min', 'acc_phone_Y_max', 'acc_phone_Y_std', 'acc_phone_Z_mean', 'acc_phone_Z_median', 'acc_phone_Z_min', 'acc_phone_Z_max', 'acc_phone_Z_std', 'lin_acc_phone_X_mean', 'lin_acc_phone_X_median', 'lin_acc_phone_X_min', 'lin_acc_phone_X_max', 'lin_acc_phone_X_std', 'lin_acc_phone_Y_mean', 'lin_acc_phone_Y_median', 'lin_acc_phone_Y_min', 'lin_acc_phone_Y_max', 'lin_acc_phone_Y_std', 'lin_acc_phone_Z_mean', 'lin_acc_phone_Z_median', 'lin_acc_phone_Z_min', 'lin_acc_phone_Z_max', 'lin_acc_phone_Z_std', 'gyr_phone_X_mean', 'gyr_phone_X_median', 'gyr_phone_X_min', 'gyr_phone_X_max', 'gyr_phone_X_std', 'gyr_phone_Y_mean', 'gyr_phone_Y_median', 'gyr_phone_Y_min', 'gyr_phone_Y_max', 'gyr_phone_Y_std', 'gyr_phone_Z_mean', 'gyr_phone_Z_median', 'gyr_phone_Z_min', 'gyr_phone_Z_max', 'gyr_phone_Z_std', 'location_phone_Velocity_mean', 'location_phone_Velocity_median', 'location_phone_Velocity_min', 'location_phone_Velocity_max', 'location_phone_Velocity_std', 'location_phone_Direction_mean', 'location_phone_Direction_median', 'location_phone_Direction_min', 'location_phone_Direction_max', 'location_phone_Direction_std', 'acc_phone_magnitude_mean', 'acc_phone_magnitude_median', 'acc_phone_magnitude_min', 'acc_phone_magnitude_max', 'acc_phone_magnitude_std', 'lin_acc_phone_magnitude_mean', 'lin_acc_phone_magnitude_median', 'lin_acc_phone_magnitude_min', 'lin_acc_phone_magnitude_max', 'lin_acc_phone_magnitude_std', 'gyr_phone_magnitude_mean', 'gyr_phone_magnitude_median', 'gyr_phone_magnitude_min', 'gyr_phone_magnitude_max', 'gyr_phone_magnitude_std', 'direction_changes_mean', 'direction_changes_median', 'direction_changes_min', 'direction_changes_max', 'direction_changes_std', 'acc_switches_mean', 'acc_switches_median', 'acc_switches_min', 'acc_switches_max', 'acc_switches_std', 'lin_acc_switches_mean', 'lin_acc_switches_median', 'lin_acc_switches_min', 'lin_acc_switches_max', 'lin_acc_switches_std', 'rotation_switches_mean', 'rotation_switches_median', 'rotation_switches_min', 'rotation_switches_max', 'rotation_switches_std', 'session_direction_change_rate', 'session_acc_switch_rate', 'session_lin_acc_switch_rate', 'session_rotation_switch_rate', 'session_avg_velocity', 'session_velocity_std']\n",
      "\n",
      "DataFrame shape: (53043, 125)\n",
      "\n",
      "Found label columns: ['labeltram', 'labeltrain', 'labelwalking', 'labelmetro', 'labelbus', 'labelcar', 'labelbike']\n",
      "Transportation modes: ['tram', 'train', 'walking', 'metro', 'bus', 'car', 'bike']\n",
      "\n",
      "All transport modes found in data:\n",
      "transport_mode\n",
      "train      15709\n",
      "walking    12412\n",
      "car         7333\n",
      "metro       6318\n",
      "bus         4946\n",
      "bike        3627\n",
      "tram        2698\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After filtering out unknown samples: 53043 samples\n",
      "Final class distribution:\n",
      "transport_mode\n",
      "train      15709\n",
      "walking    12412\n",
      "car         7333\n",
      "metro       6318\n",
      "bus         4946\n",
      "bike        3627\n",
      "tram        2698\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Session distribution analysis:\n",
      "tram sessions: 3\n",
      "train sessions: 5\n",
      "walking sessions: 8\n",
      "metro sessions: 3\n",
      "bus sessions: 3\n",
      "car sessions: 3\n",
      "bike sessions: 2\n",
      "Total unique sessions: 27\n",
      "\n",
      "⚠️  Session distribution warnings:\n",
      "WARNING: bike has only 2 session(s) - may cause issues in train/test split\n",
      "\n",
      "Using 116 features (after removing leaking features)\n",
      "Timestamp column excluded: 'timestamp'\n",
      "✅ Confirmed: No leaking features in final feature set\n",
      "\n",
      "Checking data types in features...\n",
      "Feature dtypes (first 10):\n",
      "  acc_phone_X: float64\n",
      "  acc_phone_Y: float64\n",
      "  acc_phone_Z: float64\n",
      "  lin_acc_phone_X: float64\n",
      "  gyr_phone_X: float64\n",
      "  gyr_phone_Y: float64\n",
      "  gyr_phone_Z: float64\n",
      "  mag_phone_X: float64\n",
      "  mag_phone_Y: float64\n",
      "  mag_phone_Z: float64\n",
      "Converting timedelta column 'time_diff' to seconds...\n",
      "Converting timedelta column 'shifted_time' to seconds...\n",
      "Converted 2 timedelta columns to seconds: ['time_diff', 'shifted_time']\n",
      "⚠️  WARNING: Found non-numeric columns that will be excluded: ['original_time']\n",
      "Updated feature count: 115\n",
      "\n",
      "==================================================\n",
      "IMPLEMENTING SESSION-LEVEL SPLIT FOR ALL TRANSPORT MODES\n",
      "==================================================\n",
      "⚠️  bike: Only 2 sessions - 1 train, 1 test\n",
      "\n",
      "Session split breakdown:\n",
      "  tram: 3 total -> 2 train, 1 test\n",
      "  train: 5 total -> 4 train, 1 test\n",
      "  walking: 8 total -> 6 train, 2 test\n",
      "  metro: 3 total -> 2 train, 1 test\n",
      "  bus: 3 total -> 2 train, 1 test\n",
      "  car: 3 total -> 2 train, 1 test\n",
      "  bike: 2 total -> 1 train, 1 test\n",
      "\n",
      "Total sessions:\n",
      "  Train: 19\n",
      "  Test: 8\n",
      "\n",
      "🔍 VERIFICATION:\n",
      "Train sessions: 19\n",
      "Test sessions: 8\n",
      "Session overlap: 0\n",
      "✅ SUCCESS: No session overlap between train and test!\n",
      "\n",
      "Final dataset sizes:\n",
      "Train samples: 32564\n",
      "Test samples: 20479\n",
      "Train class distribution:\n",
      "transport_mode\n",
      "train      11187\n",
      "walking     8798\n",
      "metro       3257\n",
      "car         3197\n",
      "bus         3031\n",
      "tram        1890\n",
      "bike        1204\n",
      "Name: count, dtype: int64\n",
      "Test class distribution:\n",
      "transport_mode\n",
      "train      4522\n",
      "car        4136\n",
      "walking    3614\n",
      "metro      3061\n",
      "bike       2423\n",
      "bus        1915\n",
      "tram        808\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data quality check:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in train features: 104202\n",
      "Infinite values in train features: 0\n",
      "Final feature matrix shape: Train (32564, 115), Test (20479, 115)\n",
      "\n",
      "Starting preprocessing...\n",
      "\n",
      "Label encoding for 7-class classification:\n",
      "  bike = 0\n",
      "  bus = 1\n",
      "  car = 2\n",
      "  metro = 3\n",
      "  train = 4\n",
      "  tram = 5\n",
      "  walking = 6\n",
      "Train labels distribution: [ 1204  3031  3197  3257 11187  1890  8798]\n",
      "Test labels distribution: [2423 1915 4136 3061 4522  808 3614]\n",
      "\n",
      "Training XGBoost model for 7-class classification...\n",
      "Classes: ['bike', 'bus', 'car', 'metro', 'train', 'tram', 'walking']\n",
      "[0]\ttrain-mlogloss:1.77407\teval-mlogloss:1.89353\n",
      "[19]\ttrain-mlogloss:0.53408\teval-mlogloss:1.50738\n",
      "\n",
      "Test Accuracy: 0.5839\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bike       1.00      0.74      0.85      2423\n",
      "         bus       0.00      0.00      0.00      1915\n",
      "         car       0.00      0.00      0.00      4136\n",
      "       metro       0.52      0.98      0.68      3061\n",
      "       train       1.00      1.00      1.00      4522\n",
      "        tram       0.00      0.00      0.00       808\n",
      "     walking       0.88      0.73      0.80      3614\n",
      "\n",
      "    accuracy                           0.58     20479\n",
      "   macro avg       0.49      0.49      0.48     20479\n",
      "weighted avg       0.57      0.58      0.56     20479\n",
      "\n",
      "\n",
      "Most Important Features:\n",
      "                           feature  importance\n",
      "11                    shifted_time        55.0\n",
      "69   location_phone_Direction_mean        47.0\n",
      "110        session_acc_switch_rate        41.0\n",
      "30              acc_phone_Z_median        34.0\n",
      "112   session_rotation_switch_rate        23.0\n",
      "27                 acc_phone_Y_max        20.0\n",
      "25              acc_phone_Y_median        18.0\n",
      "24                acc_phone_Y_mean        17.0\n",
      "113           session_avg_velocity        15.0\n",
      "109  session_direction_change_rate        15.0\n",
      "29                acc_phone_Z_mean        14.0\n",
      "66     location_phone_Velocity_min        13.0\n",
      "19                acc_phone_X_mean        11.0\n",
      "31                 acc_phone_Z_min         9.0\n",
      "20              acc_phone_X_median         8.0\n",
      "64    location_phone_Velocity_mean         8.0\n",
      "\n",
      "==================================================\n",
      "ADDITIONAL ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Per-class Performance Analysis:\n",
      "          bike: 0.7437 accuracy (2423 samples)\n",
      "           bus: 0.0000 accuracy (1915 samples)\n",
      "           car: 0.0000 accuracy (4136 samples)\n",
      "         metro: 0.9781 accuracy (3061 samples)\n",
      "         train: 1.0000 accuracy (4522 samples)\n",
      "          tram: 0.0000 accuracy ( 808 samples)\n",
      "       walking: 0.7305 accuracy (3614 samples)\n",
      "\n",
      "Prediction Confidence Distribution:\n",
      "Mean confidence: 0.3815\n",
      "Std confidence: 0.1361\n",
      "Min confidence: 0.1444\n",
      "Max confidence: 0.5832\n",
      "\n",
      "Sample Predictions (showing top 3 probabilities for each):\n",
      "True:        train, Pred:        train\n",
      "  Top 3: train(0.550), metro(0.126), car(0.065)\n",
      "True:          bus, Pred:        metro\n",
      "  Top 3: metro(0.245), car(0.126), bus(0.126)\n",
      "True:        train, Pred:        train\n",
      "  Top 3: train(0.550), metro(0.126), car(0.065)\n",
      "True:        metro, Pred:        metro\n",
      "  Top 3: metro(0.245), car(0.126), bus(0.126)\n",
      "True:        metro, Pred:        metro\n",
      "  Top 3: metro(0.231), car(0.179), bus(0.118)\n",
      "True:         bike, Pred:         bike\n",
      "  Top 3: bike(0.220), walking(0.139), bus(0.128)\n",
      "True:         bike, Pred:         bike\n",
      "  Top 3: bike(0.209), bus(0.183), tram(0.122)\n",
      "True:      walking, Pred:          car\n",
      "  Top 3: car(0.304), walking(0.153), bus(0.109)\n",
      "True:         bike, Pred:         bike\n",
      "  Top 3: bike(0.223), bus(0.130), tram(0.130)\n",
      "True:        train, Pred:        train\n",
      "  Top 3: train(0.550), metro(0.126), car(0.065)\n",
      "\n",
      "🎯 FINAL SUMMARY:\n",
      "Multi-class classification accuracy: 0.5839\n",
      "Number of classes: 7\n",
      "Classes: ['bike', 'bus', 'car', 'metro', 'train', 'tram', 'walking']\n",
      "Features used: 115\n",
      "Total samples: 53043 (Train: 32564, Test: 20479)\n",
      "\n",
      "⚠️  Classes that may need attention:\n",
      "  bus: 0.0000 accuracy (low performance)\n",
      "  car: 0.0000 accuracy (low performance)\n",
      "  tram: 0.0000 accuracy (low performance)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# -------------------------------\n",
    "# Load data and remove leaking features\n",
    "# -------------------------------\n",
    "df = pd.read_csv(\"/Users/yoric/ML4QS/session_features_imputed.csv\")\n",
    "\n",
    "print(\"Original DataFrame shape:\", df.shape)\n",
    "print(\"Available columns before feature removal:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Define leaking features to remove\n",
    "leaking_features = [\n",
    "    'location_phone_Latitude',\n",
    "    'location_phone_Longitude', \n",
    "    'location_phone_Direction',\n",
    "    'location_phone_Velocity',\n",
    "    'proximity_phone_Distance',\n",
    "    'location_phone_Horizontal Accuracy',\n",
    "    'location_phone_Height',\n",
    "    'location_phone_Vertical Accuracy',\n",
    "    'lin_acc_phone_Z',\n",
    "    'lin_acc_phone_Y'\n",
    "]\n",
    "\n",
    "# Remove leaking features\n",
    "print(f\"\\nRemoving {len(leaking_features)} leaking features...\")\n",
    "features_found = [f for f in leaking_features if f in df.columns]\n",
    "features_not_found = [f for f in leaking_features if f not in df.columns]\n",
    "\n",
    "if features_found:\n",
    "    print(f\"Removing features: {features_found}\")\n",
    "    df = df.drop(columns=features_found)\n",
    "else:\n",
    "    print(\"No leaking features found in the dataset\")\n",
    "\n",
    "if features_not_found:\n",
    "    print(f\"Features not found (already removed?): {features_not_found}\")\n",
    "\n",
    "print(f\"DataFrame shape after removing leaking features: {df.shape}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Inspect remaining columns\n",
    "# -------------------------------\n",
    "print(\"\\nRemaining columns:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "\n",
    "# -------------------------------\n",
    "# FIND ALL LABEL COLUMNS AUTOMATICALLY\n",
    "# -------------------------------\n",
    "# Find all label columns (assuming they start with 'label')\n",
    "label_columns = [col for col in df.columns if col.startswith('label')]\n",
    "print(f\"\\nFound label columns: {label_columns}\")\n",
    "\n",
    "# Extract transportation modes from label column names\n",
    "transportation_modes = [col.replace('label', '') for col in label_columns]\n",
    "print(f\"Transportation modes: {transportation_modes}\")\n",
    "\n",
    "def create_target_label(row):\n",
    "    \"\"\"Create target label from all available label columns\"\"\"\n",
    "    for i, col in enumerate(label_columns):\n",
    "        if row.get(col, 0) == 1:\n",
    "            return transportation_modes[i]\n",
    "    return 'unknown'\n",
    "\n",
    "df['transport_mode'] = df.apply(create_target_label, axis=1)\n",
    "\n",
    "# Show all available transport modes and their counts\n",
    "print(f\"\\nAll transport modes found in data:\")\n",
    "mode_counts = df['transport_mode'].value_counts()\n",
    "print(mode_counts)\n",
    "\n",
    "# Filter out 'unknown' samples (if any)\n",
    "df = df[df['transport_mode'] != 'unknown']\n",
    "\n",
    "print(f\"\\nAfter filtering out unknown samples: {df.shape[0]} samples\")\n",
    "print(f\"Final class distribution:\\n{df['transport_mode'].value_counts()}\")\n",
    "\n",
    "# Check session distribution by transport mode\n",
    "print(f\"\\nSession distribution analysis:\")\n",
    "session_counts = df.groupby(['id', 'transport_mode']).size().unstack(fill_value=0)\n",
    "\n",
    "sessions_per_mode = {}\n",
    "for mode in df['transport_mode'].unique():\n",
    "    sessions_with_mode = (session_counts[mode] > 0).sum() if mode in session_counts.columns else 0\n",
    "    sessions_per_mode[mode] = sessions_with_mode\n",
    "    print(f\"{mode} sessions: {sessions_with_mode}\")\n",
    "\n",
    "print(f\"Total unique sessions: {df['id'].nunique()}\")\n",
    "\n",
    "# Warn about modes with very few sessions\n",
    "print(f\"\\n⚠️  Session distribution warnings:\")\n",
    "for mode, count in sessions_per_mode.items():\n",
    "    if count < 3:\n",
    "        print(f\"WARNING: {mode} has only {count} session(s) - may cause issues in train/test split\")\n",
    "\n",
    "# -------------------------------\n",
    "# Feature selection\n",
    "# -------------------------------\n",
    "# Find timestamp column (if exists)\n",
    "time_like_columns = [col for col in df.columns if any(keyword in col.lower() for keyword in ['time', 'date', 'stamp'])]\n",
    "timestamp_col = None\n",
    "possible_timestamp_cols = ['timestamp', 'time', 'datetime', 'date_time', 'session_time']\n",
    "\n",
    "for col in possible_timestamp_cols:\n",
    "    if col in df.columns:\n",
    "        timestamp_col = col\n",
    "        break\n",
    "\n",
    "if timestamp_col is None and time_like_columns:\n",
    "    timestamp_col = time_like_columns[0]\n",
    "\n",
    "# Ensure timestamp column is excluded from features to prevent data leakage\n",
    "exclude_cols = ['id', 'transport_mode']\n",
    "if timestamp_col:\n",
    "    exclude_cols.append(timestamp_col)\n",
    "\n",
    "feature_cols = [\n",
    "    col for col in df.columns\n",
    "    if not col.startswith('label')\n",
    "    and col not in exclude_cols\n",
    "]\n",
    "\n",
    "print(f\"\\nUsing {len(feature_cols)} features (after removing leaking features)\")\n",
    "if timestamp_col:\n",
    "    print(f\"Timestamp column excluded: '{timestamp_col}'\")\n",
    "\n",
    "# Verify no leaking features remain\n",
    "remaining_leaking = [f for f in leaking_features if f in feature_cols]\n",
    "if remaining_leaking:\n",
    "    print(f\"⚠️  WARNING: Some leaking features still present: {remaining_leaking}\")\n",
    "else:\n",
    "    print(\"✅ Confirmed: No leaking features in final feature set\")\n",
    "\n",
    "# -------------------------------\n",
    "# Handle non-numeric columns (especially timedelta)\n",
    "# -------------------------------\n",
    "print(f\"\\nChecking data types in features...\")\n",
    "print(f\"Feature dtypes (first 10):\")\n",
    "for col in feature_cols[:10]:\n",
    "    if col in df.columns:\n",
    "        dtype = df[col].dtype\n",
    "        print(f\"  {col}: {dtype}\")\n",
    "        \n",
    "        if dtype == 'object':\n",
    "            sample_values = df[col].dropna().head(3).tolist()\n",
    "            print(f\"    Sample values: {sample_values}\")\n",
    "\n",
    "# Convert timedelta columns to numeric (seconds)\n",
    "timedelta_cols = []\n",
    "problematic_cols = []\n",
    "\n",
    "for col in feature_cols:\n",
    "    if col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            sample_val = str(df[col].dropna().iloc[0]) if len(df[col].dropna()) > 0 else \"\"\n",
    "            if 'days' in sample_val and ':' in sample_val:\n",
    "                print(f\"Converting timedelta column '{col}' to seconds...\")\n",
    "                try:\n",
    "                    df[col] = pd.to_timedelta(df[col]).dt.total_seconds()\n",
    "                    timedelta_cols.append(col)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to convert {col}: {e}\")\n",
    "                    problematic_cols.append(col)\n",
    "\n",
    "if timedelta_cols:\n",
    "    print(f\"Converted {len(timedelta_cols)} timedelta columns to seconds: {timedelta_cols}\")\n",
    "\n",
    "# Remove non-numeric columns\n",
    "non_numeric_cols = []\n",
    "for col in feature_cols:\n",
    "    if col in df.columns:\n",
    "        if df[col].dtype == 'object' or not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            non_numeric_cols.append(col)\n",
    "\n",
    "non_numeric_cols.extend(problematic_cols)\n",
    "non_numeric_cols = list(set(non_numeric_cols))\n",
    "\n",
    "if non_numeric_cols:\n",
    "    print(f\"⚠️  WARNING: Found non-numeric columns that will be excluded: {non_numeric_cols}\")\n",
    "    feature_cols = [col for col in feature_cols if col not in non_numeric_cols]\n",
    "    print(f\"Updated feature count: {len(feature_cols)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# IMPROVED SESSION-LEVEL SPLIT FOR ALL CLASSES\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"IMPLEMENTING SESSION-LEVEL SPLIT FOR ALL TRANSPORT MODES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get unique session IDs for each transport mode\n",
    "session_split_info = {}\n",
    "all_train_sessions = []\n",
    "all_test_sessions = []\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "for mode in df['transport_mode'].unique():\n",
    "    mode_sessions = df[df['transport_mode'] == mode]['id'].unique()\n",
    "    mode_sessions_shuffled = np.random.permutation(mode_sessions)\n",
    "    \n",
    "    n_sessions = len(mode_sessions_shuffled)\n",
    "    \n",
    "    # Handle cases with very few sessions\n",
    "    if n_sessions == 1:\n",
    "        # If only 1 session, put it in training\n",
    "        train_sessions = mode_sessions_shuffled\n",
    "        test_sessions = []\n",
    "        print(f\"⚠️  {mode}: Only 1 session - putting in training set\")\n",
    "    elif n_sessions == 2:\n",
    "        # If only 2 sessions, put 1 in each\n",
    "        train_sessions = mode_sessions_shuffled[:1]\n",
    "        test_sessions = mode_sessions_shuffled[1:]\n",
    "        print(f\"⚠️  {mode}: Only 2 sessions - 1 train, 1 test\")\n",
    "    else:\n",
    "        # Normal 80/20 split\n",
    "        split_idx = max(1, int(0.8 * n_sessions))  # Ensure at least 1 in training\n",
    "        train_sessions = mode_sessions_shuffled[:split_idx]\n",
    "        test_sessions = mode_sessions_shuffled[split_idx:]\n",
    "    \n",
    "    session_split_info[mode] = {\n",
    "        'total': n_sessions,\n",
    "        'train': len(train_sessions),\n",
    "        'test': len(test_sessions)\n",
    "    }\n",
    "    \n",
    "    all_train_sessions.extend(train_sessions)\n",
    "    all_test_sessions.extend(test_sessions)\n",
    "\n",
    "print(f\"\\nSession split breakdown:\")\n",
    "for mode, info in session_split_info.items():\n",
    "    print(f\"  {mode}: {info['total']} total -> {info['train']} train, {info['test']} test\")\n",
    "\n",
    "print(f\"\\nTotal sessions:\")\n",
    "print(f\"  Train: {len(all_train_sessions)}\")\n",
    "print(f\"  Test: {len(all_test_sessions)}\")\n",
    "\n",
    "# Create train and test datasets\n",
    "df_train = df[df['id'].isin(all_train_sessions)].copy()\n",
    "df_test = df[df['id'].isin(all_test_sessions)].copy()\n",
    "\n",
    "# Verify no session overlap\n",
    "train_session_set = set(df_train['id'].unique())\n",
    "test_session_set = set(df_test['id'].unique())\n",
    "session_overlap = train_session_set.intersection(test_session_set)\n",
    "\n",
    "print(f\"\\n🔍 VERIFICATION:\")\n",
    "print(f\"Train sessions: {len(train_session_set)}\")\n",
    "print(f\"Test sessions: {len(test_session_set)}\")\n",
    "print(f\"Session overlap: {len(session_overlap)}\")\n",
    "\n",
    "if len(session_overlap) == 0:\n",
    "    print(\"✅ SUCCESS: No session overlap between train and test!\")\n",
    "else:\n",
    "    print(\"🚨 ERROR: Session overlap detected!\")\n",
    "\n",
    "# Prepare features and targets\n",
    "X_train = df_train[feature_cols]\n",
    "X_test = df_test[feature_cols]\n",
    "y_train = df_train['transport_mode']\n",
    "y_test = df_test['transport_mode']\n",
    "\n",
    "print(f\"\\nFinal dataset sizes:\")\n",
    "print(f\"Train samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Train class distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"Test class distribution:\\n{y_test.value_counts()}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nData quality check:\")\n",
    "print(f\"Missing values in train features: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"Infinite values in train features: {np.isinf(X_train.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# Final cleanup of any remaining object columns\n",
    "object_cols = X_train.select_dtypes(include=['object']).columns\n",
    "if len(object_cols) > 0:\n",
    "    print(f\"⚠️  Removing remaining object columns: {list(object_cols)}\")\n",
    "    X_train = X_train.select_dtypes(exclude=['object'])\n",
    "    X_test = X_test.select_dtypes(exclude=['object'])\n",
    "\n",
    "print(f\"Final feature matrix shape: Train {X_train.shape}, Test {X_test.shape}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Preprocessing\n",
    "# -------------------------------\n",
    "print(f\"\\nStarting preprocessing...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Label encoding for multi-class classification\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "print(f\"\\nLabel encoding for {len(label_encoder.classes_)}-class classification:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {class_name} = {i}\")\n",
    "\n",
    "print(f\"Train labels distribution: {np.bincount(y_train_encoded)}\")\n",
    "print(f\"Test labels distribution: {np.bincount(y_test_encoded)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Train XGBoost for MULTI-CLASS Classification\n",
    "# -------------------------------\n",
    "dtrain = xgb.DMatrix(X_train_scaled, label=y_train_encoded)\n",
    "dtest = xgb.DMatrix(X_test_scaled, label=y_test_encoded)\n",
    "\n",
    "# Calculate scale_pos_weight for class imbalance (for multi-class, we'll use different approach)\n",
    "class_counts = np.bincount(y_train_encoded)\n",
    "n_classes = len(label_encoder.classes_)\n",
    "\n",
    "params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': n_classes,\n",
    "    'max_depth': 6,  # Reduced from 8 to prevent overfitting with more classes\n",
    "    'learning_rate': 0.05,  # Slightly higher learning rate\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'reg_alpha': 0.1,  # L1 regularization\n",
    "    'reg_lambda': 1.0   # L2 regularization\n",
    "}\n",
    "\n",
    "print(f\"\\nTraining XGBoost model for {n_classes}-class classification...\")\n",
    "print(f\"Classes: {list(label_encoder.classes_)}\")\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=20,  # Increased rounds for more complex problem\n",
    "    evals=[(dtrain, 'train'), (dtest, 'eval')],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluation\n",
    "# -------------------------------\n",
    "y_pred_proba = model.predict(dtest)\n",
    "y_pred_encoded = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Convert back to original labels\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_encoded)\n",
    "y_test_labels = label_encoder.inverse_transform(y_test_encoded)\n",
    "\n",
    "accuracy = accuracy_score(y_test_labels, y_pred_labels)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels, zero_division=0))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "unique_labels = sorted(label_encoder.classes_)\n",
    "cm = confusion_matrix(y_test_labels, y_pred_labels, labels=unique_labels)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=unique_labels, \n",
    "            yticklabels=unique_labels)\n",
    "plt.title(f'Confusion Matrix: {n_classes}-Class Transportation Mode Classification')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Feature Importance\n",
    "# -------------------------------\n",
    "feature_importance = model.get_score(importance_type='weight')\n",
    "final_feature_names = X_train.columns.tolist()\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': final_feature_names,\n",
    "    'importance': [feature_importance.get(f'f{i}', 0) for i in range(len(final_feature_names))]\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = importance_df.head(20)\n",
    "sns.barplot(data=top_features, x='importance', y='feature')\n",
    "plt.title(f'Top 20 Most Important Features: {n_classes}-Class Classification')\n",
    "plt.xlabel('Feature Importance (Weight)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMost Important Features:\")\n",
    "print(importance_df.head(16))\n",
    "\n",
    "# -------------------------------\n",
    "# Additional Analysis\n",
    "# -------------------------------\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"ADDITIONAL ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Per-class accuracy analysis\n",
    "print(f\"\\nPer-class Performance Analysis:\")\n",
    "for class_name in label_encoder.classes_:\n",
    "    class_mask = y_test_labels == class_name\n",
    "    n_samples = np.sum(class_mask)\n",
    "    if n_samples > 0:\n",
    "        class_accuracy = (y_pred_labels[class_mask] == class_name).mean()\n",
    "        print(f\"  {class_name:>12}: {class_accuracy:.4f} accuracy ({n_samples:>4} samples)\")\n",
    "    else:\n",
    "        print(f\"  {class_name:>12}: No test samples\")\n",
    "\n",
    "# Show prediction confidence distribution\n",
    "max_probabilities = np.max(y_pred_proba, axis=1)\n",
    "print(f\"\\nPrediction Confidence Distribution:\")\n",
    "print(f\"Mean confidence: {max_probabilities.mean():.4f}\")\n",
    "print(f\"Std confidence: {max_probabilities.std():.4f}\")\n",
    "print(f\"Min confidence: {max_probabilities.min():.4f}\")\n",
    "print(f\"Max confidence: {max_probabilities.max():.4f}\")\n",
    "\n",
    "# Sample predictions with probabilities\n",
    "print(f\"\\nSample Predictions (showing top 3 probabilities for each):\")\n",
    "sample_indices = np.random.choice(len(y_test_labels), min(10, len(y_test_labels)), replace=False)\n",
    "for i in sample_indices:\n",
    "    true_label = y_test_labels[i]\n",
    "    pred_label = y_pred_labels[i]\n",
    "    \n",
    "    # Get top 3 predictions for this sample\n",
    "    top_3_indices = np.argsort(y_pred_proba[i])[-3:][::-1]\n",
    "    top_3_probs = [(label_encoder.classes_[idx], y_pred_proba[i][idx]) for idx in top_3_indices]\n",
    "    \n",
    "    print(f\"True: {true_label:>12}, Pred: {pred_label:>12}\")\n",
    "    print(f\"  Top 3: {top_3_probs[0][0]}({top_3_probs[0][1]:.3f}), {top_3_probs[1][0]}({top_3_probs[1][1]:.3f}), {top_3_probs[2][0]}({top_3_probs[2][1]:.3f})\")\n",
    "\n",
    "print(f\"\\n🎯 FINAL SUMMARY:\")\n",
    "print(f\"Multi-class classification accuracy: {accuracy:.4f}\")\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "print(f\"Classes: {list(label_encoder.classes_)}\")\n",
    "print(f\"Features used: {X_train.shape[1]}\")\n",
    "print(f\"Total samples: {len(df)} (Train: {len(df_train)}, Test: {len(df_test)})\")\n",
    "\n",
    "# Identify problematic classes (those with very low performance)\n",
    "print(f\"\\n⚠️  Classes that may need attention:\")\n",
    "for class_name in label_encoder.classes_:\n",
    "    class_mask = y_test_labels == class_name\n",
    "    n_samples = np.sum(class_mask)\n",
    "    if n_samples > 0:\n",
    "        class_accuracy = (y_pred_labels[class_mask] == class_name).mean()\n",
    "        if class_accuracy < 0.5:\n",
    "            print(f\"  {class_name}: {class_accuracy:.4f} accuracy (low performance)\")\n",
    "        elif n_samples < 10:\n",
    "            print(f\"  {class_name}: Only {n_samples} test samples (may need more data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc3df63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312af50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we can see what the predictions on the test set actually were\n",
    "predicted_counts = pd.Series(y_pred_labels).value_counts()\n",
    "print(\"Frequency of predicted classes:\")\n",
    "print(predicted_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3c2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labels were wrong first should be ok now\n",
    "print(y_test.head())\n",
    "print(y_pred_labels[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df6df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# For a few samples\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i} predicted probabilities: {y_pred_proba[i]}\")\n",
    "    print(f\"Predicted class: {y_pred_labels[i]}, True class: {y_test.iloc[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a78bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
