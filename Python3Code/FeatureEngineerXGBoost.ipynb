{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b058571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "STUDENT = 'yoric'\n",
    "\n",
    "OUTLIERS_PATH = Path(r'C:\\Users\\yoric\\ML4QS\\ML4QS-GROUP99-Michael-FeatureEng\\Python3Code\\outliers2')\n",
    "INTERMEDIATE_PATH = Path(r'C:\\Users\\yoric\\ML4QS\\ML4QS-GROUP99-Michael-FeatureEng\\Python3Code\\intermediate_datafiles')\n",
    "\n",
    "os.chdir(r'C:\\Users\\yoric\\ML4QS\\ML4QS-GROUP99-Michael-FeatureEng\\Python3Code')\n",
    "\n",
    "EXPERIMENT_DIR = 'ML4QS-Vehicle-2'\n",
    "\n",
    "from util.VisualizeDataset import VisualizeDataset\n",
    "from Visualiser import Visualiser as Viz\n",
    "from outlier_detector import OutlierDetector\n",
    "from custom_imputer import CustomImputer\n",
    "from util.util import ignore_actual_time, read_parquet, write_parquet\n",
    "from DataLoader import PhyboxDatasetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c8696d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intermediate df\n",
    "intermediate_df = read_parquet(INTERMEDIATE_PATH / 'ML4QS_combined_results_example.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8adc513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def calculate_magnitude(df, prefix):\n",
    "    #Calculate magnitude for 3-axis sensor data: √(x² + y² + z²)\n",
    "    x_col = f\"{prefix}_X\"\n",
    "    y_col = f\"{prefix}_Y\" \n",
    "    z_col = f\"{prefix}_Z\"\n",
    "    return np.sqrt(df[x_col]**2 + df[y_col]**2 + df[z_col]**2)\n",
    "\n",
    "#Literature mentioned a window size of 120 is good for aggregating temporal features.\n",
    "#Howevr we will have to see if our data is suited for this.\n",
    "def temporal_aggregation(series, window_size=120, agg_functions=['mean', 'median', 'min', 'max', 'std']):\n",
    "    result = pd.DataFrame()\n",
    "    for func in agg_functions:\n",
    "        if func == 'std':\n",
    "            result[f\"{series.name}_{func}\"] = series.rolling(\n",
    "                window=window_size, min_periods=1\n",
    "            ).std().fillna(0)\n",
    "        else:\n",
    "            result[f\"{series.name}_{func}\"] = series.rolling(\n",
    "                window=window_size, min_periods=1\n",
    "            ).agg(func)\n",
    "    return result\n",
    "\n",
    "#I think phone_direction is where your phone is pointing, just like a google maps arrow\n",
    "#This makes sense ecause the values seem to go up to 360\n",
    "#So i define a \"direction change\" as 10degree or more\n",
    "def calculate_direction_changes(direction_series, threshold=10):\n",
    "    direction_diff = direction_series.diff().fillna(0)\n",
    "    direction_diff = np.where(direction_diff > 180, direction_diff - 360, direction_diff)\n",
    "    direction_diff = np.where(direction_diff < -180, direction_diff + 360, direction_diff)\n",
    "    significant_changes = np.abs(direction_diff) > threshold\n",
    "    return pd.Series(significant_changes.astype(int), index=direction_series.index)\n",
    "\n",
    "\n",
    "def calculate_acceleration_switches(acceleration_magnitude, threshold=0.1):\n",
    "    acc_diff = acceleration_magnitude.diff().fillna(0)\n",
    "    acc_state = np.where(acc_diff > threshold, 1,  \n",
    "                       np.where(acc_diff < -threshold, -1, 0))  \n",
    "    switches = np.abs(np.diff(acc_state, prepend=acc_state[0])) > 0\n",
    "    return pd.Series(switches.astype(int), index=acceleration_magnitude.index)\n",
    "\n",
    "def process_transportation_data(df, window_size=120):\n",
    "    \n",
    "    # Group by session ID and process each session\n",
    "    processed_sessions = []\n",
    "    \n",
    "    for session_id, session_data in df.groupby('id'):\n",
    "        session_df = session_data.copy()\n",
    "        \n",
    "        # Calculate magnitudes\n",
    "        session_df['acc_phone_magnitude'] = calculate_magnitude(session_df, 'acc_phone')\n",
    "        session_df['lin_acc_phone_magnitude'] = calculate_magnitude(session_df, 'lin_acc_phone') \n",
    "        session_df['gyr_phone_magnitude'] = calculate_magnitude(session_df, 'gyr_phone')\n",
    "        \n",
    "        # Calculate frequency-based features\n",
    "        session_df['direction_changes'] = calculate_direction_changes(session_df['location_phone_Direction'])\n",
    "        session_df['acc_switches'] = calculate_acceleration_switches(session_df['acc_phone_magnitude'])\n",
    "        session_df['lin_acc_switches'] = calculate_acceleration_switches(session_df['lin_acc_phone_magnitude'])\n",
    "        session_df['rotation_switches'] = calculate_acceleration_switches(session_df['gyr_phone_magnitude'])\n",
    "        \n",
    "        # Apply temporal aggregation to all relevant features\n",
    "        features_to_aggregate = [\n",
    "            'acc_phone_X', 'acc_phone_Y', 'acc_phone_Z',\n",
    "            'lin_acc_phone_X', 'lin_acc_phone_Y', 'lin_acc_phone_Z',\n",
    "            'gyr_phone_X', 'gyr_phone_Y', 'gyr_phone_Z',\n",
    "            'location_phone_Velocity', 'location_phone_Direction',\n",
    "            'location_phone_Horizontal Accuracy', 'location_phone_Vertical Accuracy',\n",
    "            'acc_phone_magnitude', 'lin_acc_phone_magnitude', 'gyr_phone_magnitude',\n",
    "            'direction_changes', 'acc_switches', 'lin_acc_switches', 'rotation_switches'\n",
    "        ]\n",
    "        \n",
    "        # Apply temporal aggregation\n",
    "        for feature in features_to_aggregate:\n",
    "            if feature in session_df.columns:\n",
    "                aggregated = temporal_aggregation(session_df[feature], window_size)\n",
    "                session_df = pd.concat([session_df, aggregated], axis=1)\n",
    "        \n",
    "        # Calculate session-level aggregates\n",
    "        session_direction_change_rate = session_df['direction_changes'].mean()\n",
    "        session_acc_switch_rate = session_df['acc_switches'].mean()\n",
    "        session_lin_acc_switch_rate = session_df['lin_acc_switches'].mean()\n",
    "        session_rotation_switch_rate = session_df['rotation_switches'].mean()\n",
    "        session_avg_velocity = session_df['location_phone_Velocity'].mean()\n",
    "        session_velocity_std = session_df['location_phone_Velocity'].std()\n",
    "        \n",
    "        # Add session-level features as columns to all rows in this session\n",
    "        session_df['session_direction_change_rate'] = session_direction_change_rate\n",
    "        session_df['session_acc_switch_rate'] = session_acc_switch_rate\n",
    "        session_df['session_lin_acc_switch_rate'] = session_lin_acc_switch_rate\n",
    "        session_df['session_rotation_switch_rate'] = session_rotation_switch_rate\n",
    "        session_df['session_avg_velocity'] = session_avg_velocity\n",
    "        session_df['session_velocity_std'] = session_velocity_std\n",
    "        \n",
    "        processed_sessions.append(session_df)\n",
    "    \n",
    "    # Concatenate all sessions back into one dataframe\n",
    "    result_df = pd.concat(processed_sessions, ignore_index=True)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "session_features = process_transportation_data(intermediate_df, window_size=120)\n",
    "\n",
    "session_features.to_csv('C:/Users/yoric/ML4QS/session_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "311322a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns:\n",
      "['id', 'timestamp', 'acc_phone_X', 'acc_phone_Y', 'acc_phone_Z', 'lin_acc_phone_X', 'lin_acc_phone_Y', 'lin_acc_phone_Z', 'gyr_phone_X', 'gyr_phone_Y', 'gyr_phone_Z', 'location_phone_Latitude', 'location_phone_Longitude', 'location_phone_Height', 'location_phone_Velocity', 'location_phone_Direction', 'location_phone_Horizontal Accuracy', 'location_phone_Vertical Accuracy', 'mag_phone_X', 'mag_phone_Y', 'mag_phone_Z', 'proximity_phone_Distance', 'labeltram', 'labeltrain', 'labelwalking', 'labelmetro', 'labelbus', 'labelcar', 'labelbike']\n",
      "\n",
      "DataFrame shape: (53043, 29)\n",
      "\n",
      "First few rows:\n",
      "   id               timestamp  acc_phone_X  acc_phone_Y  acc_phone_Z  \\\n",
      "0   0 2025-06-04 11:41:49.154     0.595657     3.355480     9.099531   \n",
      "1   0 2025-06-04 11:41:49.404     0.142101     3.311633     9.272894   \n",
      "2   0 2025-06-04 11:41:49.654    -0.165194     3.236582     9.294871   \n",
      "3   0 2025-06-04 11:41:49.904    -0.393992     3.161564     9.303603   \n",
      "4   0 2025-06-04 11:41:50.154    -0.447120     3.255967     9.296580   \n",
      "\n",
      "   lin_acc_phone_X  lin_acc_phone_Y  lin_acc_phone_Z  gyr_phone_X  \\\n",
      "0         0.058131        -0.000233        -0.128019    -0.087578   \n",
      "1         0.017935        -0.017567         0.049362    -0.063947   \n",
      "2         0.009399         0.005298         0.037026    -0.025154   \n",
      "3        -0.027900        -0.024366         0.029343    -0.002146   \n",
      "4         0.031913         0.007953         0.058292     0.084816   \n",
      "\n",
      "   gyr_phone_Y  ...  mag_phone_Y  mag_phone_Z  proximity_phone_Distance  \\\n",
      "0     0.173205  ...   -45.794652   -28.391914                      3.75   \n",
      "1     0.214520  ...   -44.642985   -28.854507                       NaN   \n",
      "2     0.063762  ...   -43.936887   -29.601068                       NaN   \n",
      "3     0.067268  ...   -43.764561   -29.912215                       NaN   \n",
      "4     0.035108  ...   -43.858307   -29.666903                       NaN   \n",
      "\n",
      "   labeltram  labeltrain  labelwalking  labelmetro  labelbus  labelcar  \\\n",
      "0          1           0             0           0         0         0   \n",
      "1          1           0             0           0         0         0   \n",
      "2          1           0             0           0         0         0   \n",
      "3          1           0             0           0         0         0   \n",
      "4          1           0             0           0         0         0   \n",
      "\n",
      "   labelbike  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n",
      "3          0  \n",
      "4          0  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "\n",
      "Potential time-related columns: ['timestamp']\n",
      "\n",
      "After filtering unknown transport modes: 53043 samples\n",
      "Class distribution:\n",
      "transport_mode\n",
      "train      15709\n",
      "walking    12412\n",
      "car         7333\n",
      "metro       6318\n",
      "bus         4946\n",
      "bike        3627\n",
      "tram        2698\n",
      "Name: count, dtype: int64\n",
      "\n",
      "DateTime columns found: ['timestamp']\n",
      "\n",
      "Using 20 features\n",
      "Timestamp column: 'timestamp'\n",
      "\n",
      "Train size: 42425 samples\n",
      "Test size: 10618 samples\n",
      "Class distribution (train):\n",
      "transport_mode\n",
      "train      12565\n",
      "walking     9928\n",
      "car         5865\n",
      "metro       5053\n",
      "bus         3956\n",
      "bike        2901\n",
      "tram        2157\n",
      "Name: count, dtype: int64\n",
      "Class distribution (test):\n",
      "transport_mode\n",
      "train      3144\n",
      "walking    2484\n",
      "car        1468\n",
      "metro      1265\n",
      "bus         990\n",
      "bike        726\n",
      "tram        541\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values in train features: 297308\n",
      "Infinite values in train features: 0\n",
      "\n",
      "Encoded classes: {0: 'bike', 1: 'bus', 2: 'car', 3: 'metro', 4: 'train', 5: 'tram', 6: 'walking'}\n",
      "\n",
      "Training XGBoost model...\n",
      "[0]\ttrain-mlogloss:1.91469\teval-mlogloss:1.91990\n",
      "[99]\ttrain-mlogloss:0.64192\teval-mlogloss:0.88991\n",
      "\n",
      "Test Accuracy: 0.8674\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bike       0.85      0.57      0.68       726\n",
      "         bus       0.68      0.73      0.71       990\n",
      "         car       0.99      0.99      0.99      1468\n",
      "       metro       0.97      0.89      0.93      1265\n",
      "       train       0.92      0.83      0.87      3144\n",
      "        tram       0.76      0.93      0.84       541\n",
      "     walking       0.81      0.96      0.88      2484\n",
      "\n",
      "    accuracy                           0.87     10618\n",
      "   macro avg       0.86      0.84      0.84     10618\n",
      "weighted avg       0.88      0.87      0.87     10618\n",
      "\n",
      "\n",
      "Most Important Features:\n",
      "            feature  importance\n",
      "1       acc_phone_Y      3367.0\n",
      "0       acc_phone_X      3286.0\n",
      "2       acc_phone_Z      2761.0\n",
      "17      mag_phone_Y      2311.0\n",
      "18      mag_phone_Z      2053.0\n",
      "16      mag_phone_X      1862.0\n",
      "4   lin_acc_phone_Y      1266.0\n",
      "5   lin_acc_phone_Z      1146.0\n",
      "7       gyr_phone_Y       948.0\n",
      "8       gyr_phone_Z       923.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# -------------------------------\n",
    "# Load data and inspect columns\n",
    "# -------------------------------\n",
    "df = pd.read_parquet(INTERMEDIATE_PATH / 'ML4QS_combined_results_example.parquet')\n",
    "\n",
    "# Debug: Check what columns exist\n",
    "print(\"Available columns:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Look for potential timestamp/time columns\n",
    "time_like_columns = [col for col in df.columns if any(keyword in col.lower() for keyword in ['time', 'date', 'stamp'])]\n",
    "print(f\"\\nPotential time-related columns: {time_like_columns}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Create target label\n",
    "# -------------------------------\n",
    "label_columns = ['labeltram', 'labeltrain', 'labelwalking', 'labelmetro', 'labelbus', 'labelcar', 'labelbike']\n",
    "transportation_modes = ['tram', 'train', 'walking', 'metro', 'bus', 'car', 'bike']\n",
    "\n",
    "def create_target_label(row):\n",
    "    for i, col in enumerate(label_columns):\n",
    "        if row.get(col, 0) == 1:\n",
    "            return transportation_modes[i]\n",
    "    return 'unknown'\n",
    "\n",
    "df['transport_mode'] = df.apply(create_target_label, axis=1)\n",
    "df = df[df['transport_mode'] != 'unknown'] \n",
    "\n",
    "print(f\"\\nAfter filtering unknown transport modes: {df.shape[0]} samples\")\n",
    "print(f\"Class distribution:\\n{df['transport_mode'].value_counts()}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Feature selection\n",
    "# -------------------------------\n",
    "datetime_cols = df.select_dtypes(include='datetime64').columns.tolist()\n",
    "print(f\"\\nDateTime columns found: {datetime_cols}\")\n",
    "\n",
    "# Find the correct timestamp column\n",
    "timestamp_col = None\n",
    "possible_timestamp_cols = ['timestamp', 'time', 'datetime', 'date_time', 'session_time']\n",
    "\n",
    "for col in possible_timestamp_cols:\n",
    "    if col in df.columns:\n",
    "        timestamp_col = col\n",
    "        break\n",
    "\n",
    "# If no standard timestamp column found, look for time-like columns\n",
    "if timestamp_col is None and time_like_columns:\n",
    "    timestamp_col = time_like_columns[0]  # Use the first time-like column\n",
    "    print(f\"\\nUsing '{timestamp_col}' as timestamp column\")\n",
    "\n",
    "if timestamp_col is None:\n",
    "    print(\"\\nWARNING: No timestamp column found. Options:\")\n",
    "    print(\"1. Use random split instead of temporal split\")\n",
    "    print(\"2. Create a synthetic timestamp based on row order\")\n",
    "    print(\"3. Check if your data has a different time column name\")\n",
    "    \n",
    "    # Option 2: Create synthetic timestamp based on session order\n",
    "    print(\"\\nCreating synthetic timestamp based on row order within each session...\")\n",
    "    df = df.sort_values('id').reset_index(drop=True)\n",
    "    df['synthetic_timestamp'] = df.groupby('id').cumcount()\n",
    "    timestamp_col = 'synthetic_timestamp'\n",
    "\n",
    "# Ensure timestamp column is excluded from features to prevent data leakage\n",
    "exclude_cols = ['id', 'transport_mode']\n",
    "if timestamp_col:\n",
    "    exclude_cols.append(timestamp_col)\n",
    "\n",
    "feature_cols = [\n",
    "    col for col in df.columns\n",
    "    if not col.startswith('label')\n",
    "    and col not in exclude_cols\n",
    "]\n",
    "\n",
    "print(f\"\\nUsing {len(feature_cols)} features\")\n",
    "print(f\"Timestamp column: '{timestamp_col}'\")\n",
    "\n",
    "# -------------------------------\n",
    "# Per-ID 80/20 temporal split\n",
    "# -------------------------------\n",
    "df = df.sort_values(by=['id', timestamp_col])\n",
    "\n",
    "train_parts = []\n",
    "test_parts = []\n",
    "\n",
    "for session_id, group in df.groupby('id'):\n",
    "    group_sorted = group.sort_values(timestamp_col)\n",
    "    n = len(group_sorted)\n",
    "    if n < 5:\n",
    "        continue  # skip short sessions\n",
    "    split_idx = int(n * 0.8)\n",
    "    train_parts.append(group_sorted.iloc[:split_idx])\n",
    "    test_parts.append(group_sorted.iloc[split_idx:])\n",
    "\n",
    "if not train_parts or not test_parts:\n",
    "    raise ValueError(\"No data after filtering short sessions. Check your session lengths.\")\n",
    "\n",
    "df_train = pd.concat(train_parts).reset_index(drop=True)\n",
    "df_test = pd.concat(test_parts).reset_index(drop=True)\n",
    "\n",
    "X_train = df_train[feature_cols]\n",
    "X_test = df_test[feature_cols]\n",
    "y_train = df_train['transport_mode']\n",
    "y_test = df_test['transport_mode']\n",
    "\n",
    "print(f\"\\nTrain size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test size: {X_test.shape[0]} samples\")\n",
    "print(f\"Class distribution (train):\\n{y_train.value_counts()}\")\n",
    "print(f\"Class distribution (test):\\n{y_test.value_counts()}\")\n",
    "\n",
    "# Check for any missing values or infinite values\n",
    "print(f\"\\nMissing values in train features: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"Infinite values in train features: {np.isinf(X_train.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Preprocessing\n",
    "# -------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "y_train_cat = pd.Categorical(y_train)\n",
    "y_test_cat = pd.Categorical(y_test, categories=y_train_cat.categories) \n",
    "\n",
    "y_train_encoded = y_train_cat.codes\n",
    "y_test_encoded = y_test_cat.codes\n",
    "\n",
    "print(f\"\\nEncoded classes: {dict(enumerate(y_train_cat.categories))}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Train XGBoost\n",
    "# -------------------------------\n",
    "dtrain = xgb.DMatrix(X_train_scaled, label=y_train_encoded)\n",
    "dtest = xgb.DMatrix(X_test_scaled, label=y_test_encoded)\n",
    "\n",
    "params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': len(y_train_cat.categories),\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.01,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'eval_metric': 'mlogloss'\n",
    "}\n",
    "\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=100,\n",
    "    evals=[(dtrain, 'train'), (dtest, 'eval')],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=100\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluation\n",
    "# -------------------------------\n",
    "y_pred_proba = model.predict(dtest)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "y_pred_labels = [y_train_cat.categories[i] for i in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_labels))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, y_pred_labels, labels=y_train_cat.categories)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=y_train_cat.categories, yticklabels=y_train_cat.categories)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Feature Importance\n",
    "# -------------------------------\n",
    "feature_importance = model.get_score(importance_type='weight')\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': [feature_importance.get(f'f{i}', 0) for i in range(len(feature_cols))]\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=importance_df.head(20), x='importance', y='feature')\n",
    "plt.title('Top 20 Most Important Features')\n",
    "plt.xlabel('Feature Importance (Weight)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMost Important Features:\")\n",
    "print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bc3df63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA LEAKAGE DEBUGGING ===\n",
      "\n",
      "1. CHECKING FEATURE NAMES FOR SUSPICIOUS PATTERNS:\n",
      "--------------------------------------------------\n",
      "No obviously suspicious feature names found\n",
      "\n",
      "2. CHECKING TRAIN-TEST SESSION OVERLAP:\n",
      "--------------------------------------------------\n",
      "Train sessions: 27\n",
      "Test sessions: 27\n",
      "OVERLAPPING: 27\n",
      "Overlap percentage: 100.0%\n",
      "Same sessions appear in both train and test!\n",
      "This causes severe data leakage!\n",
      "\n",
      "3. CHECKING FEATURE DISTRIBUTIONS:\n",
      "--------------------------------------------------\n",
      "\n",
      "4. CHECKING CLASS DISTRIBUTIONS PER SESSION:\n",
      "--------------------------------------------------\n",
      "Sessions with only one transport mode: 27/27\n",
      "Percentage of 'pure' sessions: 100.0%\n",
      "Most sessions have only one transport mode!\n",
      "This makes the temporal split essentially a session-level split,\n",
      "which might be too easy for the model.\n",
      "\n",
      "5. CHECKING FOR PERFECT SEPARABILITY:\n",
      "--------------------------------------------------\n",
      "Simple decision tree accuracy: 0.7874\n",
      "\n",
      "6. EXAMINING TOP FEATURES:\n",
      "--------------------------------------------------\n",
      "\n",
      "session_avg_velocity:\n",
      "                  mean    std    min     max\n",
      "transport_mode                              \n",
      "bike             3.698  0.292  3.493   4.112\n",
      "bus              8.698  2.292  5.503  11.232\n",
      "car             13.954  4.392  8.153  17.794\n",
      "metro            8.098  0.782  7.369   9.033\n",
      "train            0.283  0.000  0.283   0.283\n",
      "tram             4.491  0.447  4.030   5.091\n",
      "walking          0.974  0.306  0.553   1.442\n",
      "\n",
      "session_acc_switch_rate:\n",
      "                 mean    std    min    max\n",
      "transport_mode                            \n",
      "bike            0.561  0.016  0.549  0.583\n",
      "bus             0.627  0.049  0.583  0.684\n",
      "car             0.690  0.016  0.677  0.719\n",
      "metro           0.305  0.029  0.276  0.335\n",
      "train           0.574  0.073  0.351  0.654\n",
      "tram            0.338  0.050  0.278  0.394\n",
      "walking         0.888  0.043  0.805  0.932\n",
      "\n",
      "session_velocity_std:\n",
      "                 mean    std    min    max\n",
      "transport_mode                            \n",
      "bike            1.274  0.141  1.075  1.373\n",
      "bus             4.829  0.344  4.141  5.104\n",
      "car             6.253  1.278  4.217  7.333\n",
      "metro           7.856  1.506  4.232  9.054\n",
      "train           0.067  0.000  0.067  0.067\n",
      "tram            3.864  0.364  3.605  4.452\n",
      "walking         0.572  0.094  0.439  0.837\n",
      "\n",
      "=== RECOMMENDATIONS ===\n",
      "1. If sessions overlap between train/test: Use proper session-level split\n",
      "2. If features are too discriminative: Check feature engineering process\n",
      "3. If sessions are 'pure': Consider if this reflects real-world usage\n",
      "4. Validate with cross-validation across different sessions\n",
      "5. Test on completely new data from different time periods/users\n"
     ]
    }
   ],
   "source": [
    "# Debug script to identify potential data leakage issues\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data again for debugging\n",
    "df = pd.read_csv(\"/Users/yoric/ML4QS/session_features.csv\")\n",
    "\n",
    "print(\"=== DATA LEAKAGE DEBUGGING ===\\n\")\n",
    "\n",
    "# 1. Check for potential leakage in feature names\n",
    "print(\"1. CHECKING FEATURE NAMES FOR SUSPICIOUS PATTERNS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "suspicious_keywords = ['label', 'target', 'class', 'mode', 'transport', 'activity', 'ground_truth']\n",
    "feature_cols = [col for col in df.columns if not col.startswith('label') \n",
    "                and col not in ['id', 'transport_mode']]\n",
    "\n",
    "suspicious_features = []\n",
    "for feature in feature_cols:\n",
    "    for keyword in suspicious_keywords:\n",
    "        if keyword in feature.lower():\n",
    "            suspicious_features.append(feature)\n",
    "            break\n",
    "\n",
    "if suspicious_features:\n",
    "    print(f\"{suspicious_features}\")\n",
    "else:\n",
    "    print(\"No obviously suspicious feature names found\")\n",
    "\n",
    "# 2. Check train-test overlap by session IDs\n",
    "print(f\"\\n2. CHECKING TRAIN-TEST SESSION OVERLAP:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Recreate the split to check overlap\n",
    "label_columns = ['labeltram', 'labeltrain', 'labelwalking', 'labelmetro', 'labelbus', 'labelcar', 'labelbike']\n",
    "transportation_modes = ['tram', 'train', 'walking', 'metro', 'bus', 'car', 'bike']\n",
    "\n",
    "def create_target_label(row):\n",
    "    for i, col in enumerate(label_columns):\n",
    "        if row.get(col, 0) == 1:\n",
    "            return transportation_modes[i]\n",
    "    return 'unknown'\n",
    "\n",
    "df['transport_mode'] = df.apply(create_target_label, axis=1)\n",
    "df = df[df['transport_mode'] != 'unknown']\n",
    "\n",
    "# Find timestamp column\n",
    "timestamp_col = None\n",
    "possible_timestamp_cols = ['timestamp', 'time', 'datetime', 'date_time', 'session_time']\n",
    "time_like_columns = [col for col in df.columns if any(keyword in col.lower() for keyword in ['time', 'date', 'stamp'])]\n",
    "\n",
    "for col in possible_timestamp_cols:\n",
    "    if col in df.columns:\n",
    "        timestamp_col = col\n",
    "        break\n",
    "\n",
    "if timestamp_col is None and time_like_columns:\n",
    "    timestamp_col = time_like_columns[0]\n",
    "\n",
    "if timestamp_col is None:\n",
    "    df = df.sort_values('id').reset_index(drop=True)\n",
    "    df['synthetic_timestamp'] = df.groupby('id').cumcount()\n",
    "    timestamp_col = 'synthetic_timestamp'\n",
    "\n",
    "# Recreate the split\n",
    "df = df.sort_values(by=['id', timestamp_col])\n",
    "train_parts = []\n",
    "test_parts = []\n",
    "\n",
    "for session_id, group in df.groupby('id'):\n",
    "    group_sorted = group.sort_values(timestamp_col)\n",
    "    n = len(group_sorted)\n",
    "    if n < 5:\n",
    "        continue\n",
    "    split_idx = int(n * 0.8)\n",
    "    train_parts.append(group_sorted.iloc[:split_idx])\n",
    "    test_parts.append(group_sorted.iloc[split_idx:])\n",
    "\n",
    "df_train = pd.concat(train_parts).reset_index(drop=True)\n",
    "df_test = pd.concat(test_parts).reset_index(drop=True)\n",
    "\n",
    "train_sessions = set(df_train['id'].unique())\n",
    "test_sessions = set(df_test['id'].unique())\n",
    "session_overlap = train_sessions.intersection(test_sessions)\n",
    "\n",
    "print(f\"Train sessions: {len(train_sessions)}\")\n",
    "print(f\"Test sessions: {len(test_sessions)}\")\n",
    "print(f\": {len(session_overlap)}\")\n",
    "print(f\"Overlap percentage: {len(session_overlap)/len(train_sessions)*100:.1f}%\")\n",
    "\n",
    "if len(session_overlap) > 0:\n",
    "    print(\" Same sessions appear in both train and test!\")\n",
    "    print(\"This causes severe data leakage!\")\n",
    "\n",
    "# 3. Check feature distributions between train and test\n",
    "print(f\"\\n3. CHECKING FEATURE DISTRIBUTIONS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "feature_cols = [col for col in df.columns if not col.startswith('label') \n",
    "                and col not in ['id', 'transport_mode', timestamp_col]]\n",
    "\n",
    "X_train = df_train[feature_cols]\n",
    "X_test = df_test[feature_cols]\n",
    "\n",
    "# Check for identical distributions (another leakage sign)\n",
    "suspicious_identical_features = []\n",
    "for col in feature_cols[:10]:  # Check first 10 features\n",
    "    if col in X_train.columns and col in X_test.columns:\n",
    "        train_vals = X_train[col].dropna()\n",
    "        test_vals = X_test[col].dropna()\n",
    "        \n",
    "        if len(train_vals) > 0 and len(test_vals) > 0:\n",
    "            # Check if distributions are suspiciously similar\n",
    "            train_mean = train_vals.mean()\n",
    "            test_mean = test_vals.mean()\n",
    "            train_std = train_vals.std()\n",
    "            test_std = test_vals.std()\n",
    "            \n",
    "            if (abs(train_mean - test_mean) < 0.001 and \n",
    "                abs(train_std - test_std) < 0.001):\n",
    "                suspicious_identical_features.append(col)\n",
    "\n",
    "if suspicious_identical_features:\n",
    "    print(f\"Features with identical distributions: {suspicious_identical_features[:5]}\")\n",
    "\n",
    "# 4. Check class distribution per session\n",
    "print(f\"\\n4. CHECKING CLASS DISTRIBUTIONS PER SESSION:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "session_class_counts = df.groupby(['id', 'transport_mode']).size().unstack(fill_value=0)\n",
    "pure_sessions = []\n",
    "\n",
    "for session_id in session_class_counts.index:\n",
    "    non_zero_classes = (session_class_counts.loc[session_id] > 0).sum()\n",
    "    if non_zero_classes == 1:  # Only one transport mode per session\n",
    "        pure_sessions.append(session_id)\n",
    "\n",
    "print(f\"Sessions with only one transport mode: {len(pure_sessions)}/{len(session_class_counts)}\")\n",
    "print(f\"Percentage of 'pure' sessions: {len(pure_sessions)/len(session_class_counts)*100:.1f}%\")\n",
    "\n",
    "if len(pure_sessions) / len(session_class_counts) > 0.8:\n",
    "    print(\"Most sessions have only one transport mode!\")\n",
    "    print(\"This makes the temporal split essentially a session-level split,\")\n",
    "    print(\"which might be too easy for the model.\")\n",
    "\n",
    "# 5. Check for perfect separability features\n",
    "print(f\"\\n5. CHECKING FOR PERFECT SEPARABILITY:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Quick check if a simple decision tree can achieve perfect accuracy\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dt.fit(X_train.fillna(0), df_train['transport_mode'])\n",
    "dt_score = dt.score(X_test.fillna(0), df_test['transport_mode'])\n",
    "\n",
    "print(f\"Simple decision tree accuracy: {dt_score:.4f}\")\n",
    "if dt_score > 0.95:\n",
    "    print(\"Even a simple decision tree gets >95% accuracy!\")\n",
    "    print(\"This suggests the problem might be too easy or has leakage.\")\n",
    "\n",
    "# 6. Show sample of the most important features\n",
    "print(f\"\\n6. EXAMINING TOP FEATURES:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "top_features = ['session_avg_velocity', 'session_acc_switch_rate', 'session_velocity_std']\n",
    "for feature in top_features:\n",
    "    if feature in df.columns:\n",
    "        print(f\"\\n{feature}:\")\n",
    "        feature_by_mode = df.groupby('transport_mode')[feature].agg(['mean', 'std', 'min', 'max'])\n",
    "        print(feature_by_mode.round(3))\n",
    "\n",
    "print(f\"\\n=== RECOMMENDATIONS ===\")\n",
    "print(\"1. If sessions overlap between train/test: Use proper session-level split\")\n",
    "print(\"2. If features are too discriminative: Check feature engineering process\")\n",
    "print(\"3. If sessions are 'pure': Consider if this reflects real-world usage\")\n",
    "print(\"4. Validate with cross-validation across different sessions\")\n",
    "print(\"5. Test on completely new data from different time periods/users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312af50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we can see what the predictions on the test set actually were\n",
    "predicted_counts = pd.Series(y_pred_labels).value_counts()\n",
    "print(\"Frequency of predicted classes:\")\n",
    "print(predicted_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3c2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labels were wrong first should be ok now\n",
    "print(y_test.head())\n",
    "print(y_pred_labels[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df6df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# For a few samples\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i} predicted probabilities: {y_pred_proba[i]}\")\n",
    "    print(f\"Predicted class: {y_pred_labels[i]}, True class: {y_test.iloc[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a78bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
